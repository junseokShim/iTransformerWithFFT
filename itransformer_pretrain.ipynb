{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('dataset/merged_df.csv')\n",
    "metrics_train = pd.read_csv('dataset/ch2025_data_items/ch2025_metrics_train.csv')\n",
    "sample_submission = pd.read_csv('dataset/ch2025_submission_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = merged_df.subject_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_data(metrics_train, merged_df):\n",
    "    metrics_train['lifelog_date'] = pd.to_datetime(metrics_train['lifelog_date']).dt.date\n",
    "    merged_df['date'] = pd.to_datetime(merged_df['date']).dt.date\n",
    "\n",
    "    metrics_train = metrics_train.rename(columns={'lifelog_date': 'date'})\n",
    "\n",
    "    train_df = pd.merge(metrics_train, merged_df, on=['subject_id', 'date'], how='inner')\n",
    "\n",
    "    merged_keys = merged_df[['subject_id', 'date']]\n",
    "    train_keys = metrics_train[['subject_id', 'date']]\n",
    "    test_keys = pd.merge(merged_keys, train_keys, on=['subject_id', 'date'], how='left', indicator=True)\n",
    "    test_keys = test_keys[test_keys['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "    test_df = pd.merge(test_keys, merged_df, on=['subject_id', 'date'], how='left')\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def generate_submission(sample_submission, binary_preds, multiclass_pred, filename):\n",
    "    sample_submission['lifelog_date'] = pd.to_datetime(sample_submission['lifelog_date']).dt.date\n",
    "    submission_final = sample_submission[['subject_id', 'sleep_date', 'lifelog_date']].copy()\n",
    "    submission_final['ID'] = submission_final['subject_id'] + '_' + submission_final['lifelog_date'].astype(str)\n",
    "\n",
    "    submission_final['S1'] = multiclass_pred\n",
    "    for col in binary_preds:\n",
    "        submission_final[col] = binary_preds[col].astype(int)\n",
    "\n",
    "    submission_final = submission_final[['subject_id', 'sleep_date', 'lifelog_date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']]\n",
    "    submission_final.to_csv(filename, index=False)\n",
    "    print(f\"✅ 제출 파일 생성 완료: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = prepare_train_test_data(metrics_train, merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_binary = ['Q1', 'Q2', 'Q3', 'S2', 'S3']\n",
    "target_multiclass = 'S1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=['sleep_date', 'date'] + targets_binary + [target_multiclass]).fillna(0)\n",
    "\n",
    "Y = train_df[['subject_id'] + targets_binary + [target_multiclass]].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 14, 126) (310, 6)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 14 # Best : 14\n",
    "\n",
    "X_seq = []\n",
    "Y_seq = []\n",
    "\n",
    "for user in users:\n",
    "    for i in range(X[X.subject_id==user].shape[0]-seq_len):\n",
    "        X_seq.append(X[X.subject_id==user].iloc[i:i+seq_len, 1:].to_numpy())\n",
    "        Y_seq.append(Y[Y.subject_id==user].iloc[i+seq_len, 1:])\n",
    "    \n",
    "X_seq = np.array(X_seq)\n",
    "#Y_seq = np.array(Y_seq)\n",
    "\n",
    "print(X_seq.shape, np.array(Y_seq).shape)\n",
    "\n",
    "X_seq_len = X_seq.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "test_X_seq = []\n",
    "\n",
    "for user in users:\n",
    "    for date in sample_submission[sample_submission.subject_id == user].lifelog_date:\n",
    "        c_index = merged_df[(merged_df['subject_id'] == user) & (merged_df['date'] == dt.datetime.strptime(date, \"%Y-%m-%d\").date())].index\n",
    "        test_X_seq.append(merged_df.iloc[c_index[0]-seq_len:c_index[0], :].drop(columns=['subject_id', 'date']).fillna(0).to_numpy())\n",
    "        \n",
    "test_X_seq = np.array(test_X_seq)\n",
    "test_X_seq_len = test_X_seq.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 14, 126)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "# Example: Check the shape of X_seq\n",
    "print(X_seq.shape)\n",
    "\n",
    "# If X_seq is 3D, reshape it to 2D\n",
    "if len(X_seq.shape) == 3:\n",
    "    X_seq = X_seq.reshape(-1, X_seq.shape[-1])\n",
    "    test_X_seq = test_X_seq.reshape(-1, test_X_seq.shape[-1])\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_seq)\n",
    "X_train = X_train_scaled.reshape(X_seq_len, seq_len, 126)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "\n",
    "X_test_scaled = scaler.transform(test_X_seq)\n",
    "X_test = X_test_scaled.reshape(test_X_seq_len, seq_len, 126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([310, 14, 126]) torch.Size([310])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j1/l91_nhvj6jddv1qmvcx7cfkc0000gn/T/ipykernel_75384/2700865825.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.8167, Avg Val F1 Score: 0.5581\n",
      "✅ Best model saved for Q1\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 1.0645, Avg Val F1 Score: 0.5854\n",
      "✅ Best model saved for Q1\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 1.0478, Avg Val F1 Score: 0.5811\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 1.0291, Avg Val F1 Score: 0.6205\n",
      "✅ Best model saved for Q1\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 1.1112, Avg Val F1 Score: 0.5631\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 1.5918, Avg Val F1 Score: 0.4425\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 1.057, Avg Val F1 Score: 0.576\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 1.1392, Avg Val F1 Score: 0.5785\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.993, Avg Val F1 Score: 0.548\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 1.2872, Avg Val F1 Score: 0.5494\n",
      "Epoch: 11, Avg Val Loss: 1.3433, Avg Val F1 Score: 0.5466\n",
      "Epoch: 12, Avg Val Loss: 0.7958, Avg Val F1 Score: 0.6293\n",
      "✅ Best model saved for Q1\n",
      "Epoch: 13, Avg Val Loss: 1.1177, Avg Val F1 Score: 0.5509\n",
      "Epoch: 14, Avg Val Loss: 1.2206, Avg Val F1 Score: 0.5713\n",
      "Epoch: 15, Avg Val Loss: 1.0218, Avg Val F1 Score: 0.6052\n",
      "Epoch: 16, Avg Val Loss: 1.2067, Avg Val F1 Score: 0.5906\n",
      "Epoch: 17, Avg Val Loss: 1.1332, Avg Val F1 Score: 0.5328\n",
      "Epoch: 18, Avg Val Loss: 1.2656, Avg Val F1 Score: 0.5732\n",
      "Epoch: 19, Avg Val Loss: 1.6445, Avg Val F1 Score: 0.5045\n",
      "Epoch: 20, Avg Val Loss: 1.4097, Avg Val F1 Score: 0.4995\n",
      "Epoch: 21, Avg Val Loss: 1.4496, Avg Val F1 Score: 0.5412\n",
      "Epoch: 22, Avg Val Loss: 1.2277, Avg Val F1 Score: 0.5744\n",
      "Epoch: 23, Avg Val Loss: 1.2935, Avg Val F1 Score: 0.5728\n",
      "Epoch: 24, Avg Val Loss: 1.3863, Avg Val F1 Score: 0.5869\n",
      "Epoch: 25, Avg Val Loss: 1.3444, Avg Val F1 Score: 0.5382\n",
      "Epoch: 26, Avg Val Loss: 1.4768, Avg Val F1 Score: 0.5736\n",
      "Epoch: 27, Avg Val Loss: 2.0421, Avg Val F1 Score: 0.5397\n",
      "Early stopping triggered.\n",
      "torch.Size([310, 14, 126]) torch.Size([310])\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.7445, Avg Val F1 Score: 0.6033\n",
      "✅ Best model saved for Q2\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.9272, Avg Val F1 Score: 0.5457\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.8432, Avg Val F1 Score: 0.6579\n",
      "✅ Best model saved for Q2\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.8735, Avg Val F1 Score: 0.6509\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.9405, Avg Val F1 Score: 0.6543\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 1.2112, Avg Val F1 Score: 0.5058\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 1.3504, Avg Val F1 Score: 0.4987\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.9756, Avg Val F1 Score: 0.643\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 1.2893, Avg Val F1 Score: 0.6202\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.9827, Avg Val F1 Score: 0.6197\n",
      "Epoch: 11, Avg Val Loss: 1.5789, Avg Val F1 Score: 0.5239\n",
      "Epoch: 12, Avg Val Loss: 1.1726, Avg Val F1 Score: 0.6217\n",
      "Epoch: 13, Avg Val Loss: 1.1756, Avg Val F1 Score: 0.6032\n",
      "Epoch: 14, Avg Val Loss: 1.5013, Avg Val F1 Score: 0.6161\n",
      "Epoch: 15, Avg Val Loss: 1.3167, Avg Val F1 Score: 0.5792\n",
      "Epoch: 16, Avg Val Loss: 1.5904, Avg Val F1 Score: 0.5077\n",
      "Epoch: 17, Avg Val Loss: 1.4296, Avg Val F1 Score: 0.5887\n",
      "Epoch: 18, Avg Val Loss: 2.1992, Avg Val F1 Score: 0.4599\n",
      "Early stopping triggered.\n",
      "torch.Size([310, 14, 126]) torch.Size([310])\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.6326, Avg Val F1 Score: 0.7114\n",
      "✅ Best model saved for Q3\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.6344, Avg Val F1 Score: 0.6732\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.6293, Avg Val F1 Score: 0.6597\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.7455, Avg Val F1 Score: 0.6364\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.7253, Avg Val F1 Score: 0.6568\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.7656, Avg Val F1 Score: 0.6449\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 1.0424, Avg Val F1 Score: 0.6054\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.6846, Avg Val F1 Score: 0.6924\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.8202, Avg Val F1 Score: 0.644\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 1.5172, Avg Val F1 Score: 0.4366\n",
      "Epoch: 11, Avg Val Loss: 0.8151, Avg Val F1 Score: 0.6344\n",
      "Epoch: 12, Avg Val Loss: 0.8664, Avg Val F1 Score: 0.7048\n",
      "Epoch: 13, Avg Val Loss: 0.8439, Avg Val F1 Score: 0.5741\n",
      "Epoch: 14, Avg Val Loss: 0.6694, Avg Val F1 Score: 0.6685\n",
      "Epoch: 15, Avg Val Loss: 0.7573, Avg Val F1 Score: 0.6627\n",
      "Epoch: 16, Avg Val Loss: 0.7069, Avg Val F1 Score: 0.6878\n",
      "Early stopping triggered.\n",
      "torch.Size([310, 14, 126]) torch.Size([310])\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.8298, Avg Val F1 Score: 0.4913\n",
      "✅ Best model saved for S2\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.8271, Avg Val F1 Score: 0.6065\n",
      "✅ Best model saved for S2\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.7445, Avg Val F1 Score: 0.6685\n",
      "✅ Best model saved for S2\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.9281, Avg Val F1 Score: 0.5117\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.7333, Avg Val F1 Score: 0.6435\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.8524, Avg Val F1 Score: 0.6155\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.861, Avg Val F1 Score: 0.6412\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.8058, Avg Val F1 Score: 0.5888\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 1.1281, Avg Val F1 Score: 0.5876\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 1.1343, Avg Val F1 Score: 0.5348\n",
      "Epoch: 11, Avg Val Loss: 0.646, Avg Val F1 Score: 0.6571\n",
      "Epoch: 12, Avg Val Loss: 0.7698, Avg Val F1 Score: 0.6448\n",
      "Epoch: 13, Avg Val Loss: 0.829, Avg Val F1 Score: 0.6574\n",
      "Epoch: 14, Avg Val Loss: 0.8761, Avg Val F1 Score: 0.6285\n",
      "Epoch: 15, Avg Val Loss: 1.0001, Avg Val F1 Score: 0.5746\n",
      "Epoch: 16, Avg Val Loss: 0.7466, Avg Val F1 Score: 0.7044\n",
      "✅ Best model saved for S2\n",
      "Epoch: 17, Avg Val Loss: 0.8673, Avg Val F1 Score: 0.6087\n",
      "Epoch: 18, Avg Val Loss: 0.7628, Avg Val F1 Score: 0.6389\n",
      "Epoch: 19, Avg Val Loss: 0.934, Avg Val F1 Score: 0.5893\n",
      "Epoch: 20, Avg Val Loss: 0.8319, Avg Val F1 Score: 0.6648\n",
      "Epoch: 21, Avg Val Loss: 1.0219, Avg Val F1 Score: 0.6107\n",
      "Epoch: 22, Avg Val Loss: 0.9347, Avg Val F1 Score: 0.6177\n",
      "Epoch: 23, Avg Val Loss: 0.975, Avg Val F1 Score: 0.636\n",
      "Epoch: 24, Avg Val Loss: 1.1538, Avg Val F1 Score: 0.5685\n",
      "Epoch: 25, Avg Val Loss: 0.9925, Avg Val F1 Score: 0.6509\n",
      "Epoch: 26, Avg Val Loss: 1.2537, Avg Val F1 Score: 0.6011\n",
      "Epoch: 27, Avg Val Loss: 1.2559, Avg Val F1 Score: 0.6358\n",
      "Epoch: 28, Avg Val Loss: 1.5649, Avg Val F1 Score: 0.5362\n",
      "Epoch: 29, Avg Val Loss: 1.294, Avg Val F1 Score: 0.5639\n",
      "Epoch: 30, Avg Val Loss: 1.0411, Avg Val F1 Score: 0.5768\n",
      "Epoch: 31, Avg Val Loss: 1.1155, Avg Val F1 Score: 0.5662\n",
      "Early stopping triggered.\n",
      "torch.Size([310, 14, 126]) torch.Size([310])\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.5196, Avg Val F1 Score: 0.7464\n",
      "✅ Best model saved for S3\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.5507, Avg Val F1 Score: 0.7248\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.5929, Avg Val F1 Score: 0.7052\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.8235, Avg Val F1 Score: 0.555\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.757, Avg Val F1 Score: 0.6722\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.5326, Avg Val F1 Score: 0.6513\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.5787, Avg Val F1 Score: 0.7005\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.5783, Avg Val F1 Score: 0.7027\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.9881, Avg Val F1 Score: 0.6216\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.6347, Avg Val F1 Score: 0.6771\n",
      "Epoch: 11, Avg Val Loss: 0.6193, Avg Val F1 Score: 0.6362\n",
      "Epoch: 12, Avg Val Loss: 0.7775, Avg Val F1 Score: 0.6747\n",
      "Epoch: 13, Avg Val Loss: 0.7606, Avg Val F1 Score: 0.6941\n",
      "Epoch: 14, Avg Val Loss: 0.5952, Avg Val F1 Score: 0.6642\n",
      "Epoch: 15, Avg Val Loss: 1.1076, Avg Val F1 Score: 0.5935\n",
      "Epoch: 16, Avg Val Loss: 0.9161, Avg Val F1 Score: 0.6536\n",
      "Early stopping triggered.\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 1.0875, Avg Val F1 Score: 0.4801\n",
      "✅ Best model saved for S1\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 1.4034, Avg Val F1 Score: 0.4272\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 1.2167, Avg Val F1 Score: 0.4822\n",
      "✅ Best model saved for S1\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 1.2392, Avg Val F1 Score: 0.4442\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 1.8873, Avg Val F1 Score: 0.3485\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 1.5674, Avg Val F1 Score: 0.4544\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 1.5736, Avg Val F1 Score: 0.4786\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 1.5979, Avg Val F1 Score: 0.3433\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 1.2848, Avg Val F1 Score: 0.49\n",
      "✅ Best model saved for S1\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 1.5766, Avg Val F1 Score: 0.3925\n",
      "Epoch: 11, Avg Val Loss: 1.8279, Avg Val F1 Score: 0.4383\n",
      "Epoch: 12, Avg Val Loss: 1.9456, Avg Val F1 Score: 0.3889\n",
      "Epoch: 13, Avg Val Loss: 1.3916, Avg Val F1 Score: 0.4608\n",
      "Epoch: 14, Avg Val Loss: 1.6952, Avg Val F1 Score: 0.4081\n",
      "Epoch: 15, Avg Val Loss: 1.5972, Avg Val F1 Score: 0.4389\n",
      "Epoch: 16, Avg Val Loss: 1.839, Avg Val F1 Score: 0.3555\n",
      "Epoch: 17, Avg Val Loss: 2.1463, Avg Val F1 Score: 0.3876\n",
      "Epoch: 18, Avg Val Loss: 1.6603, Avg Val F1 Score: 0.4287\n",
      "Epoch: 19, Avg Val Loss: 1.6262, Avg Val F1 Score: 0.4516\n",
      "Epoch: 20, Avg Val Loss: 1.5444, Avg Val F1 Score: 0.4111\n",
      "Epoch: 21, Avg Val Loss: 1.4434, Avg Val F1 Score: 0.5316\n",
      "✅ Best model saved for S1\n",
      "Epoch: 22, Avg Val Loss: 1.5892, Avg Val F1 Score: 0.4237\n",
      "Epoch: 23, Avg Val Loss: 2.1394, Avg Val F1 Score: 0.3723\n",
      "Epoch: 24, Avg Val Loss: 1.7435, Avg Val F1 Score: 0.459\n",
      "Epoch: 25, Avg Val Loss: 2.0125, Avg Val F1 Score: 0.3786\n",
      "Epoch: 26, Avg Val Loss: 1.7729, Avg Val F1 Score: 0.4756\n",
      "Epoch: 27, Avg Val Loss: 1.85, Avg Val F1 Score: 0.3996\n",
      "Epoch: 28, Avg Val Loss: 2.1545, Avg Val F1 Score: 0.4108\n",
      "Epoch: 29, Avg Val Loss: 2.2972, Avg Val F1 Score: 0.4262\n",
      "Epoch: 30, Avg Val Loss: 1.9077, Avg Val F1 Score: 0.4744\n",
      "Epoch: 31, Avg Val Loss: 1.8957, Avg Val F1 Score: 0.4444\n",
      "Epoch: 32, Avg Val Loss: 2.161, Avg Val F1 Score: 0.4233\n",
      "Epoch: 33, Avg Val Loss: 2.1073, Avg Val F1 Score: 0.4476\n",
      "Epoch: 34, Avg Val Loss: 2.1038, Avg Val F1 Score: 0.4381\n",
      "Epoch: 35, Avg Val Loss: 2.1823, Avg Val F1 Score: 0.4084\n",
      "Epoch: 36, Avg Val Loss: 1.8868, Avg Val F1 Score: 0.4954\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "from models.iftransformer import *\n",
    "from train import *\n",
    "from prediction import *\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "seed_value = 42\n",
    "\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "binary_preds = {}\n",
    "binary_f1 = {}\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "hidden_dim =512 # Best : 256\n",
    "num_heads =64 # Best : 64\n",
    "n_layers = 6 # Best : 6\n",
    "\n",
    "epoch = 100 # Best : 10\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "\n",
    "for col in targets_binary:\n",
    "    y_train = torch.tensor(np.array([[y[col]] for y in Y_seq]), dtype=torch.long).view(-1)\n",
    "    \n",
    "    print(X_train.shape, y_train.shape)\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    model_bin = IFTransformer(input_dim=X_train.shape[-1], num_heads = num_heads, n_layers = n_layers, num_classes=2, hidden_dim=int(hidden_dim), dropout=0.1)\n",
    "    binary_f1[col] = train_model(model_bin, dataloader, nn.CrossEntropyLoss(), optim.Adam(model_bin.parameters(), lr=0.0001), col = col, epochs=epoch) # Best lr : 0.0001\n",
    "    binary_preds[col] = predict(model_bin, torch.tensor(X_test, dtype=torch.float32), col)\n",
    "    f1_scores.append(binary_f1[col])\n",
    "\n",
    "y_multi_train = torch.tensor(np.array([[y[target_multiclass]] for y in Y_seq]), dtype=torch.long).view(-1)\n",
    "dataset_multi = TensorDataset(X_train, y_multi_train)\n",
    "dataloader_multi = DataLoader(dataset_multi, batch_size=32, shuffle=True)\n",
    "\n",
    "model_multi = IFTransformer(input_dim=X_train.shape[-1], num_heads = num_heads, n_layers = n_layers, num_classes=3, hidden_dim=int(hidden_dim), dropout=0.1)\n",
    "multiclass_f1 = train_model(model_multi, dataloader_multi, nn.CrossEntropyLoss(), optim.Adam(model_multi.parameters(), lr=0.0001), col = 'S1', epochs=epoch) # Best lr : 0.0001\n",
    "f1_scores.append(multiclass_f1)\n",
    "\n",
    "multiclass_pred = predict(model_multi, torch.tensor(X_test, dtype=torch.float32), 'S1')\n",
    "avg_f1 = sum(f1_scores) / len(f1_scores)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 제출 파일 생성 완료: submission_IFTransformer_1024_head_256_num_layer_12_time_2025-05-19 23:06:41.023351_seq_14_epoch_100_val_0.2.csv\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "\n",
    "generate_submission(sample_submission, binary_preds, multiclass_pred, f'submission_IFTransformer_{hidden_dim}_head_{num_heads}_num_layer_{n_layers}_time_{current_time}_seq_{seq_len}_epoch_{epoch}_val_0.2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6634791677409732"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
