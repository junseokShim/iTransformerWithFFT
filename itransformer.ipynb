{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('dataset/merged_df.csv')\n",
    "metrics_train = pd.read_csv('dataset/ch2025_data_items/ch2025_metrics_train.csv')\n",
    "sample_submission = pd.read_csv('dataset/ch2025_submission_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = merged_df.subject_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_data(metrics_train, merged_df):\n",
    "    metrics_train['lifelog_date'] = pd.to_datetime(metrics_train['lifelog_date']).dt.date\n",
    "    merged_df['date'] = pd.to_datetime(merged_df['date']).dt.date\n",
    "\n",
    "    metrics_train = metrics_train.rename(columns={'lifelog_date': 'date'})\n",
    "\n",
    "    train_df = pd.merge(metrics_train, merged_df, on=['subject_id', 'date'], how='inner')\n",
    "\n",
    "    merged_keys = merged_df[['subject_id', 'date']]\n",
    "    train_keys = metrics_train[['subject_id', 'date']]\n",
    "    test_keys = pd.merge(merged_keys, train_keys, on=['subject_id', 'date'], how='left', indicator=True)\n",
    "    test_keys = test_keys[test_keys['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "    test_df = pd.merge(test_keys, merged_df, on=['subject_id', 'date'], how='left')\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def generate_submission(sample_submission, binary_preds, multiclass_pred, filename):\n",
    "    sample_submission['lifelog_date'] = pd.to_datetime(sample_submission['lifelog_date']).dt.date\n",
    "    submission_final = sample_submission[['subject_id', 'sleep_date', 'lifelog_date']].copy()\n",
    "    submission_final['ID'] = submission_final['subject_id'] + '_' + submission_final['lifelog_date'].astype(str)\n",
    "\n",
    "    submission_final['S1'] = multiclass_pred\n",
    "    for col in binary_preds:\n",
    "        submission_final[col] = binary_preds[col].astype(int)\n",
    "\n",
    "    submission_final = submission_final[['subject_id', 'sleep_date', 'lifelog_date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']]\n",
    "    submission_final.to_csv(filename, index=False)\n",
    "    print(f\"✅ 제출 파일 생성 완료: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = prepare_train_test_data(metrics_train, merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_binary = ['Q1', 'Q2', 'Q3', 'S2', 'S3']\n",
    "target_multiclass = 'S1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=['sleep_date', 'date'] + targets_binary + [target_multiclass]).fillna(0)\n",
    "\n",
    "Y = train_df[['subject_id'] + targets_binary + [target_multiclass]].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 14, 126) (310, 6)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 14 # Best : 14\n",
    "\n",
    "X_seq = []\n",
    "Y_seq = []\n",
    "\n",
    "for user in users:\n",
    "    for i in range(X[X.subject_id==user].shape[0]-seq_len):\n",
    "        X_seq.append(X[X.subject_id==user].iloc[i:i+seq_len, 1:].to_numpy())\n",
    "        Y_seq.append(Y[Y.subject_id==user].iloc[i+seq_len, 1:])\n",
    "    \n",
    "X_seq = np.array(X_seq)\n",
    "#Y_seq = np.array(Y_seq)\n",
    "\n",
    "print(X_seq.shape, np.array(Y_seq).shape)\n",
    "\n",
    "X_seq_len = X_seq.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "test_X_seq = []\n",
    "\n",
    "for user in users:\n",
    "    for date in sample_submission[sample_submission.subject_id == user].lifelog_date:\n",
    "        c_index = merged_df[(merged_df['subject_id'] == user) & (merged_df['date'] == dt.datetime.strptime(date, \"%Y-%m-%d\").date())].index\n",
    "        test_X_seq.append(merged_df.iloc[c_index[0]-seq_len:c_index[0], :].drop(columns=['subject_id', 'date']).fillna(0).to_numpy())\n",
    "        \n",
    "test_X_seq = np.array(test_X_seq)\n",
    "test_X_seq_len = test_X_seq.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 14, 126)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "# Example: Check the shape of X_seq\n",
    "print(X_seq.shape)\n",
    "\n",
    "# If X_seq is 3D, reshape it to 2D\n",
    "if len(X_seq.shape) == 3:\n",
    "    X_seq = X_seq.reshape(-1, X_seq.shape[-1])\n",
    "    test_X_seq = test_X_seq.reshape(-1, test_X_seq.shape[-1])\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_seq)\n",
    "X_train = X_train_scaled.reshape(X_seq_len, seq_len, 126)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "\n",
    "X_test_scaled = scaler.transform(test_X_seq)\n",
    "X_test = X_test_scaled.reshape(test_X_seq_len, seq_len, 126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([310, 14, 126]) torch.Size([310])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j1/l91_nhvj6jddv1qmvcx7cfkc0000gn/T/ipykernel_69224/1321559705.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.7036, Avg Val F1 Score: 0.5403\n",
      "✅ Best model saved for Q1\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.6641, Avg Val F1 Score: 0.6644\n",
      "✅ Best model saved for Q1\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.628, Avg Val F1 Score: 0.7201\n",
      "✅ Best model saved for Q1\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.5938, Avg Val F1 Score: 0.7334\n",
      "✅ Best model saved for Q1\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.5977, Avg Val F1 Score: 0.6675\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.5886, Avg Val F1 Score: 0.7107\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.5933, Avg Val F1 Score: 0.7752\n",
      "✅ Best model saved for Q1\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.6103, Avg Val F1 Score: 0.7382\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.6417, Avg Val F1 Score: 0.7521\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.6752, Avg Val F1 Score: 0.6528\n",
      "Epoch: 11, Avg Val Loss: 0.677, Avg Val F1 Score: 0.7103\n",
      "Epoch: 12, Avg Val Loss: 0.7389, Avg Val F1 Score: 0.7159\n",
      "Epoch: 13, Avg Val Loss: 0.8291, Avg Val F1 Score: 0.6243\n",
      "Epoch: 14, Avg Val Loss: 0.7761, Avg Val F1 Score: 0.6564\n",
      "Epoch: 15, Avg Val Loss: 0.7706, Avg Val F1 Score: 0.6641\n",
      "Epoch: 16, Avg Val Loss: 0.8282, Avg Val F1 Score: 0.6124\n",
      "Epoch: 17, Avg Val Loss: 0.6831, Avg Val F1 Score: 0.6648\n",
      "Epoch: 18, Avg Val Loss: 0.8254, Avg Val F1 Score: 0.5403\n",
      "Epoch: 19, Avg Val Loss: 0.7959, Avg Val F1 Score: 0.6471\n",
      "Epoch: 20, Avg Val Loss: 0.8103, Avg Val F1 Score: 0.6827\n",
      "Epoch: 21, Avg Val Loss: 0.8465, Avg Val F1 Score: 0.6999\n",
      "Epoch: 22, Avg Val Loss: 0.9286, Avg Val F1 Score: 0.7093\n",
      "Early stopping triggered.\n",
      "torch.Size([310, 14, 126]) torch.Size([310])\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.6756, Avg Val F1 Score: 0.5253\n",
      "✅ Best model saved for Q2\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.6107, Avg Val F1 Score: 0.615\n",
      "✅ Best model saved for Q2\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.5844, Avg Val F1 Score: 0.6722\n",
      "✅ Best model saved for Q2\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.562, Avg Val F1 Score: 0.7072\n",
      "✅ Best model saved for Q2\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.56, Avg Val F1 Score: 0.7015\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.5635, Avg Val F1 Score: 0.7015\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.5599, Avg Val F1 Score: 0.6922\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.5793, Avg Val F1 Score: 0.7015\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.6073, Avg Val F1 Score: 0.7451\n",
      "✅ Best model saved for Q2\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.6195, Avg Val F1 Score: 0.7423\n",
      "Epoch: 11, Avg Val Loss: 0.7078, Avg Val F1 Score: 0.6562\n",
      "Epoch: 12, Avg Val Loss: 0.6322, Avg Val F1 Score: 0.7143\n",
      "Epoch: 13, Avg Val Loss: 0.6146, Avg Val F1 Score: 0.6833\n",
      "Epoch: 14, Avg Val Loss: 0.6336, Avg Val F1 Score: 0.6889\n",
      "Epoch: 15, Avg Val Loss: 0.5952, Avg Val F1 Score: 0.7414\n",
      "Epoch: 16, Avg Val Loss: 0.6463, Avg Val F1 Score: 0.6889\n",
      "Epoch: 17, Avg Val Loss: 0.684, Avg Val F1 Score: 0.7015\n",
      "Epoch: 18, Avg Val Loss: 0.727, Avg Val F1 Score: 0.6434\n",
      "Epoch: 19, Avg Val Loss: 0.7357, Avg Val F1 Score: 0.7274\n",
      "Epoch: 20, Avg Val Loss: 0.8933, Avg Val F1 Score: 0.6833\n",
      "Epoch: 21, Avg Val Loss: 0.7975, Avg Val F1 Score: 0.6951\n",
      "Epoch: 22, Avg Val Loss: 0.7115, Avg Val F1 Score: 0.6722\n",
      "Epoch: 23, Avg Val Loss: 0.8247, Avg Val F1 Score: 0.7104\n",
      "Epoch: 24, Avg Val Loss: 0.7917, Avg Val F1 Score: 0.7018\n",
      "Early stopping triggered.\n",
      "torch.Size([310, 14, 126]) torch.Size([310])\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.7383, Avg Val F1 Score: 0.376\n",
      "✅ Best model saved for Q3\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.6715, Avg Val F1 Score: 0.5778\n",
      "✅ Best model saved for Q3\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.6546, Avg Val F1 Score: 0.6058\n",
      "✅ Best model saved for Q3\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.6349, Avg Val F1 Score: 0.6463\n",
      "✅ Best model saved for Q3\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.6231, Avg Val F1 Score: 0.6659\n",
      "✅ Best model saved for Q3\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.6424, Avg Val F1 Score: 0.6933\n",
      "✅ Best model saved for Q3\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.6654, Avg Val F1 Score: 0.7068\n",
      "✅ Best model saved for Q3\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.7434, Avg Val F1 Score: 0.6556\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.7189, Avg Val F1 Score: 0.6803\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.9097, Avg Val F1 Score: 0.5975\n",
      "Epoch: 11, Avg Val Loss: 0.8775, Avg Val F1 Score: 0.6077\n",
      "Epoch: 12, Avg Val Loss: 1.0836, Avg Val F1 Score: 0.5513\n",
      "Epoch: 13, Avg Val Loss: 0.948, Avg Val F1 Score: 0.67\n",
      "Epoch: 14, Avg Val Loss: 0.91, Avg Val F1 Score: 0.6333\n",
      "Epoch: 15, Avg Val Loss: 0.9363, Avg Val F1 Score: 0.5943\n",
      "Epoch: 16, Avg Val Loss: 0.9329, Avg Val F1 Score: 0.6009\n",
      "Epoch: 17, Avg Val Loss: 0.9395, Avg Val F1 Score: 0.6111\n",
      "Epoch: 18, Avg Val Loss: 0.9183, Avg Val F1 Score: 0.6445\n",
      "Epoch: 19, Avg Val Loss: 1.0922, Avg Val F1 Score: 0.6646\n",
      "Epoch: 20, Avg Val Loss: 1.089, Avg Val F1 Score: 0.6685\n",
      "Epoch: 21, Avg Val Loss: 1.3652, Avg Val F1 Score: 0.6119\n",
      "Epoch: 22, Avg Val Loss: 1.1705, Avg Val F1 Score: 0.6434\n",
      "Early stopping triggered.\n",
      "torch.Size([310, 14, 126]) torch.Size([310])\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.6533, Avg Val F1 Score: 0.6192\n",
      "✅ Best model saved for S2\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.637, Avg Val F1 Score: 0.5724\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.6249, Avg Val F1 Score: 0.6729\n",
      "✅ Best model saved for S2\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.5784, Avg Val F1 Score: 0.6677\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.5745, Avg Val F1 Score: 0.6767\n",
      "✅ Best model saved for S2\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.6041, Avg Val F1 Score: 0.6767\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.5722, Avg Val F1 Score: 0.6797\n",
      "✅ Best model saved for S2\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.6002, Avg Val F1 Score: 0.6891\n",
      "✅ Best model saved for S2\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.5981, Avg Val F1 Score: 0.68\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.6354, Avg Val F1 Score: 0.6648\n",
      "Epoch: 11, Avg Val Loss: 0.6663, Avg Val F1 Score: 0.7255\n",
      "✅ Best model saved for S2\n",
      "Epoch: 12, Avg Val Loss: 0.7292, Avg Val F1 Score: 0.6574\n",
      "Epoch: 13, Avg Val Loss: 0.7453, Avg Val F1 Score: 0.7463\n",
      "✅ Best model saved for S2\n",
      "Epoch: 14, Avg Val Loss: 0.6872, Avg Val F1 Score: 0.7265\n",
      "Epoch: 15, Avg Val Loss: 0.7881, Avg Val F1 Score: 0.6844\n",
      "Epoch: 16, Avg Val Loss: 0.7284, Avg Val F1 Score: 0.7255\n",
      "Epoch: 17, Avg Val Loss: 0.7404, Avg Val F1 Score: 0.6861\n",
      "Epoch: 18, Avg Val Loss: 0.9178, Avg Val F1 Score: 0.6322\n",
      "Epoch: 19, Avg Val Loss: 0.8155, Avg Val F1 Score: 0.7164\n",
      "Epoch: 20, Avg Val Loss: 0.8488, Avg Val F1 Score: 0.7019\n",
      "Epoch: 21, Avg Val Loss: 0.8487, Avg Val F1 Score: 0.6474\n",
      "Epoch: 22, Avg Val Loss: 0.7463, Avg Val F1 Score: 0.7728\n",
      "✅ Best model saved for S2\n",
      "Epoch: 23, Avg Val Loss: 1.154, Avg Val F1 Score: 0.6568\n",
      "Epoch: 24, Avg Val Loss: 0.9356, Avg Val F1 Score: 0.6847\n",
      "Epoch: 25, Avg Val Loss: 0.902, Avg Val F1 Score: 0.7029\n",
      "Epoch: 26, Avg Val Loss: 0.8374, Avg Val F1 Score: 0.7354\n",
      "Epoch: 27, Avg Val Loss: 0.9017, Avg Val F1 Score: 0.6864\n",
      "Epoch: 28, Avg Val Loss: 0.9636, Avg Val F1 Score: 0.6912\n",
      "Epoch: 29, Avg Val Loss: 0.9911, Avg Val F1 Score: 0.7331\n",
      "Epoch: 30, Avg Val Loss: 1.0662, Avg Val F1 Score: 0.6929\n",
      "Epoch: 31, Avg Val Loss: 1.237, Avg Val F1 Score: 0.7321\n",
      "Epoch: 32, Avg Val Loss: 1.1592, Avg Val F1 Score: 0.6775\n",
      "Epoch: 33, Avg Val Loss: 1.3123, Avg Val F1 Score: 0.6929\n",
      "Epoch: 34, Avg Val Loss: 1.1067, Avg Val F1 Score: 0.7492\n",
      "Epoch: 35, Avg Val Loss: 1.1705, Avg Val F1 Score: 0.6669\n",
      "Epoch: 36, Avg Val Loss: 1.1502, Avg Val F1 Score: 0.7325\n",
      "Epoch: 37, Avg Val Loss: 1.0715, Avg Val F1 Score: 0.6712\n",
      "Early stopping triggered.\n",
      "torch.Size([310, 14, 126]) torch.Size([310])\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.6183, Avg Val F1 Score: 0.6538\n",
      "✅ Best model saved for S3\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.574, Avg Val F1 Score: 0.6428\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.5638, Avg Val F1 Score: 0.6985\n",
      "✅ Best model saved for S3\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.5679, Avg Val F1 Score: 0.7268\n",
      "✅ Best model saved for S3\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.5901, Avg Val F1 Score: 0.7247\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.6036, Avg Val F1 Score: 0.7108\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.6186, Avg Val F1 Score: 0.7108\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.6755, Avg Val F1 Score: 0.6168\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.6621, Avg Val F1 Score: 0.6993\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.6641, Avg Val F1 Score: 0.6619\n",
      "Epoch: 11, Avg Val Loss: 0.6901, Avg Val F1 Score: 0.6211\n",
      "Epoch: 12, Avg Val Loss: 0.7002, Avg Val F1 Score: 0.7004\n",
      "Epoch: 13, Avg Val Loss: 0.7489, Avg Val F1 Score: 0.6881\n",
      "Epoch: 14, Avg Val Loss: 0.7471, Avg Val F1 Score: 0.7194\n",
      "Epoch: 15, Avg Val Loss: 0.7639, Avg Val F1 Score: 0.6845\n",
      "Epoch: 16, Avg Val Loss: 0.715, Avg Val F1 Score: 0.6477\n",
      "Epoch: 17, Avg Val Loss: 0.8942, Avg Val F1 Score: 0.6457\n",
      "Epoch: 18, Avg Val Loss: 0.8158, Avg Val F1 Score: 0.7229\n",
      "Epoch: 19, Avg Val Loss: 0.8146, Avg Val F1 Score: 0.6936\n",
      "Early stopping triggered.\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 1.0427, Avg Val F1 Score: 0.4363\n",
      "✅ Best model saved for S1\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.9935, Avg Val F1 Score: 0.4903\n",
      "✅ Best model saved for S1\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.9965, Avg Val F1 Score: 0.4774\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 1.0021, Avg Val F1 Score: 0.4508\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 1.0125, Avg Val F1 Score: 0.4562\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 1.0613, Avg Val F1 Score: 0.4098\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 1.1254, Avg Val F1 Score: 0.3864\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 1.1282, Avg Val F1 Score: 0.4083\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 1.1958, Avg Val F1 Score: 0.4403\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 1.2776, Avg Val F1 Score: 0.3642\n",
      "Epoch: 11, Avg Val Loss: 1.3331, Avg Val F1 Score: 0.4702\n",
      "Epoch: 12, Avg Val Loss: 1.2984, Avg Val F1 Score: 0.4368\n",
      "Epoch: 13, Avg Val Loss: 1.4695, Avg Val F1 Score: 0.4203\n",
      "Epoch: 14, Avg Val Loss: 1.4053, Avg Val F1 Score: 0.4249\n",
      "Epoch: 15, Avg Val Loss: 1.526, Avg Val F1 Score: 0.4395\n",
      "Epoch: 16, Avg Val Loss: 1.5387, Avg Val F1 Score: 0.4358\n",
      "Epoch: 17, Avg Val Loss: 1.6365, Avg Val F1 Score: 0.4181\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "from models.iftransformer import *\n",
    "from train import *\n",
    "from prediction import *\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "seed_value = 42\n",
    "\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "binary_preds = {}\n",
    "binary_f1 = {}\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "hidden_dim = 256 # Best : 256\n",
    "num_heads = 64 # Best : 64\n",
    "n_layers = 6 # Best : 6\n",
    "\n",
    "epoch = 100 # Best : 100\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "\n",
    "for col in targets_binary:\n",
    "    y_train = torch.tensor(np.array([[y[col]] for y in Y_seq]), dtype=torch.long).view(-1)\n",
    "    \n",
    "    print(X_train.shape, y_train.shape)\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    model_bin = IFTransformerClassifier(input_dim=X_train.shape[-1], num_heads = num_heads, n_layers = n_layers, num_classes=2, hidden_dim=int(hidden_dim))\n",
    "    binary_f1[col] = train_model(model_bin, dataloader, nn.CrossEntropyLoss(), optim.Adam(model_bin.parameters(), lr=0.0001), col = col, epochs=epoch) # Best lr : 0.0001\n",
    "    binary_preds[col] = predict(model_bin, torch.tensor(X_test, dtype=torch.float32), col)\n",
    "    f1_scores.append(binary_f1[col])\n",
    "\n",
    "y_multi_train = torch.tensor(np.array([[y[target_multiclass]] for y in Y_seq]), dtype=torch.long).view(-1)\n",
    "dataset_multi = TensorDataset(X_train, y_multi_train)\n",
    "dataloader_multi = DataLoader(dataset_multi, batch_size=32, shuffle=True)\n",
    "\n",
    "model_multi = IFTransformerClassifier(input_dim=X_train.shape[-1], num_heads = num_heads, n_layers = n_layers, num_classes=3, hidden_dim=int(hidden_dim))\n",
    "multiclass_f1 = train_model(model_multi, dataloader_multi, nn.CrossEntropyLoss(), optim.Adam(model_multi.parameters(), lr=0.0001), col = 'S1', epochs=epoch) # Best lr : 0.0001\n",
    "f1_scores.append(multiclass_f1)\n",
    "\n",
    "multiclass_pred = predict(model_multi, torch.tensor(X_test, dtype=torch.float32), 'S1')\n",
    "avg_f1 = sum(f1_scores) / len(f1_scores)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 제출 파일 생성 완료: submission_iftransformer1_modify_256_head_64_num_layer_6_time_2025-05-18 21:50:25.844312_seq_14_epoch_100_val_0.2.csv\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "\n",
    "generate_submission(sample_submission, binary_preds, multiclass_pred, f'submission_iftransformer1_modify_{hidden_dim}_head_{num_heads}_num_layer_{n_layers}_time_{current_time}_seq_{seq_len}_epoch_{epoch}_val_0.2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7028275781776427"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
