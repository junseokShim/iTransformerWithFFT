{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('dataset/merged_df.csv')\n",
    "metrics_train = pd.read_csv('dataset/ch2025_data_items/ch2025_metrics_train.csv')\n",
    "sample_submission = pd.read_csv('dataset/ch2025_submission_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = merged_df.subject_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_data(metrics_train, merged_df):\n",
    "    metrics_train['lifelog_date'] = pd.to_datetime(metrics_train['lifelog_date']).dt.date\n",
    "    merged_df['date'] = pd.to_datetime(merged_df['date']).dt.date\n",
    "\n",
    "    metrics_train = metrics_train.rename(columns={'lifelog_date': 'date'})\n",
    "\n",
    "    train_df = pd.merge(metrics_train, merged_df, on=['subject_id', 'date'], how='inner')\n",
    "\n",
    "    merged_keys = merged_df[['subject_id', 'date']]\n",
    "    train_keys = metrics_train[['subject_id', 'date']]\n",
    "    test_keys = pd.merge(merged_keys, train_keys, on=['subject_id', 'date'], how='left', indicator=True)\n",
    "    test_keys = test_keys[test_keys['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "    test_df = pd.merge(test_keys, merged_df, on=['subject_id', 'date'], how='left')\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def generate_submission(sample_submission, binary_preds, multiclass_pred, filename):\n",
    "    sample_submission['lifelog_date'] = pd.to_datetime(sample_submission['lifelog_date']).dt.date\n",
    "    submission_final = sample_submission[['subject_id', 'sleep_date', 'lifelog_date']].copy()\n",
    "    submission_final['ID'] = submission_final['subject_id'] + '_' + submission_final['lifelog_date'].astype(str)\n",
    "\n",
    "    submission_final['S1'] = multiclass_pred\n",
    "    for col in binary_preds:\n",
    "        submission_final[col] = binary_preds[col].astype(int)\n",
    "\n",
    "    submission_final = submission_final[['subject_id', 'sleep_date', 'lifelog_date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']]\n",
    "    submission_final.to_csv(filename, index=False)\n",
    "    print(f\"✅ 제출 파일 생성 완료: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = prepare_train_test_data(metrics_train, merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_binary = ['Q1', 'Q2', 'Q3', 'S2', 'S3']\n",
    "target_multiclass = 'S1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=['sleep_date', 'date'] + targets_binary + [target_multiclass]).fillna(0)\n",
    "\n",
    "Y = train_df[['subject_id'] + targets_binary + [target_multiclass]].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 14, 126) (310, 6)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 14\n",
    "\n",
    "X_seq = []\n",
    "Y_seq = []\n",
    "\n",
    "for user in users:\n",
    "    for i in range(X[X.subject_id==user].shape[0]-seq_len):\n",
    "        X_seq.append(X[X.subject_id==user].iloc[i:i+seq_len, 1:].to_numpy())\n",
    "        Y_seq.append(Y[Y.subject_id==user].iloc[i+seq_len, 1:])\n",
    "    \n",
    "X_seq = np.array(X_seq)\n",
    "#Y_seq = np.array(Y_seq)\n",
    "\n",
    "print(X_seq.shape, np.array(Y_seq).shape)\n",
    "\n",
    "X_seq_len = X_seq.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "test_X_seq = []\n",
    "\n",
    "for user in users:\n",
    "    for date in sample_submission[sample_submission.subject_id == user].lifelog_date:\n",
    "        c_index = merged_df[(merged_df['subject_id'] == user) & (merged_df['date'] == dt.datetime.strptime(date, \"%Y-%m-%d\").date())].index\n",
    "        test_X_seq.append(merged_df.iloc[c_index[0]-seq_len:c_index[0], :].drop(columns=['subject_id', 'date']).fillna(0).to_numpy())\n",
    "        \n",
    "test_X_seq = np.array(test_X_seq)\n",
    "test_X_seq_len = test_X_seq.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 14, 126)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "# Example: Check the shape of X_seq\n",
    "print(X_seq.shape)\n",
    "\n",
    "# If X_seq is 3D, reshape it to 2D\n",
    "if len(X_seq.shape) == 3:\n",
    "    X_seq = X_seq.reshape(-1, X_seq.shape[-1])\n",
    "    test_X_seq = test_X_seq.reshape(-1, test_X_seq.shape[-1])\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_seq)\n",
    "X_train = X_train_scaled.reshape(X_seq_len, seq_len, 126)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "\n",
    "X_test_scaled = scaler.transform(test_X_seq)\n",
    "X_test = X_test_scaled.reshape(test_X_seq_len, seq_len, 126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([310, 14, 126]) torch.Size([310])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j1/l91_nhvj6jddv1qmvcx7cfkc0000gn/T/ipykernel_46496/2878537102.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "/var/folders/j1/l91_nhvj6jddv1qmvcx7cfkc0000gn/T/ipykernel_46496/2878537102.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.6837, Avg Val F1 Score: 0.39\n",
      "✅ Best model saved for Q1\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.6734, Avg Val F1 Score: 0.4318\n",
      "✅ Best model saved for Q1\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.6624, Avg Val F1 Score: 0.4925\n",
      "✅ Best model saved for Q1\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.6522, Avg Val F1 Score: 0.5107\n",
      "✅ Best model saved for Q1\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.6443, Avg Val F1 Score: 0.5669\n",
      "✅ Best model saved for Q1\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.6381, Avg Val F1 Score: 0.6235\n",
      "✅ Best model saved for Q1\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.637, Avg Val F1 Score: 0.6402\n",
      "✅ Best model saved for Q1\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.6356, Avg Val F1 Score: 0.6203\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.6372, Avg Val F1 Score: 0.6627\n",
      "✅ Best model saved for Q1\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.6371, Avg Val F1 Score: 0.6181\n",
      "Epoch: 11, Avg Val Loss: 0.6387, Avg Val F1 Score: 0.6162\n",
      "Epoch: 12, Avg Val Loss: 0.6429, Avg Val F1 Score: 0.6039\n",
      "Epoch: 13, Avg Val Loss: 0.6491, Avg Val F1 Score: 0.5943\n",
      "Epoch: 14, Avg Val Loss: 0.6557, Avg Val F1 Score: 0.6508\n",
      "Epoch: 15, Avg Val Loss: 0.6573, Avg Val F1 Score: 0.5832\n",
      "Epoch: 16, Avg Val Loss: 0.6599, Avg Val F1 Score: 0.5619\n",
      "Epoch: 17, Avg Val Loss: 0.6634, Avg Val F1 Score: 0.5619\n",
      "Epoch: 18, Avg Val Loss: 0.6607, Avg Val F1 Score: 0.6181\n",
      "Epoch: 19, Avg Val Loss: 0.6673, Avg Val F1 Score: 0.6181\n",
      "Epoch: 20, Avg Val Loss: 0.6798, Avg Val F1 Score: 0.6192\n",
      "Epoch: 21, Avg Val Loss: 0.6871, Avg Val F1 Score: 0.5978\n",
      "Epoch: 22, Avg Val Loss: 0.6975, Avg Val F1 Score: 0.5869\n",
      "Epoch: 23, Avg Val Loss: 0.7189, Avg Val F1 Score: 0.5869\n",
      "Epoch: 24, Avg Val Loss: 0.7301, Avg Val F1 Score: 0.5869\n",
      "Early stopping triggered.\n",
      "torch.Size([310, 14, 126]) torch.Size([310])\n",
      "Epoch 1: Warm-up LR = 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j1/l91_nhvj6jddv1qmvcx7cfkc0000gn/T/ipykernel_46496/2878537102.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Avg Val Loss: 0.6931, Avg Val F1 Score: 0.4549\n",
      "✅ Best model saved for Q2\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.6907, Avg Val F1 Score: 0.4476\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.6871, Avg Val F1 Score: 0.4195\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.685, Avg Val F1 Score: 0.4195\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.6822, Avg Val F1 Score: 0.4381\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.6795, Avg Val F1 Score: 0.4381\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.6773, Avg Val F1 Score: 0.4729\n",
      "✅ Best model saved for Q2\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.6754, Avg Val F1 Score: 0.4854\n",
      "✅ Best model saved for Q2\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.6708, Avg Val F1 Score: 0.6342\n",
      "✅ Best model saved for Q2\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.6704, Avg Val F1 Score: 0.6023\n",
      "Epoch: 11, Avg Val Loss: 0.6753, Avg Val F1 Score: 0.6173\n",
      "Epoch: 12, Avg Val Loss: 0.6761, Avg Val F1 Score: 0.6339\n",
      "Epoch: 13, Avg Val Loss: 0.6788, Avg Val F1 Score: 0.6494\n",
      "✅ Best model saved for Q2\n",
      "Epoch: 14, Avg Val Loss: 0.6824, Avg Val F1 Score: 0.6444\n",
      "Epoch: 15, Avg Val Loss: 0.6821, Avg Val F1 Score: 0.6444\n",
      "Epoch: 16, Avg Val Loss: 0.6824, Avg Val F1 Score: 0.658\n",
      "✅ Best model saved for Q2\n",
      "Epoch: 17, Avg Val Loss: 0.6824, Avg Val F1 Score: 0.658\n",
      "Epoch: 18, Avg Val Loss: 0.6827, Avg Val F1 Score: 0.649\n",
      "Epoch: 19, Avg Val Loss: 0.6896, Avg Val F1 Score: 0.64\n",
      "Epoch: 20, Avg Val Loss: 0.6899, Avg Val F1 Score: 0.6331\n",
      "Epoch: 21, Avg Val Loss: 0.6916, Avg Val F1 Score: 0.64\n",
      "Epoch: 22, Avg Val Loss: 0.6913, Avg Val F1 Score: 0.6424\n",
      "Epoch: 23, Avg Val Loss: 0.7, Avg Val F1 Score: 0.6424\n",
      "Epoch: 24, Avg Val Loss: 0.7036, Avg Val F1 Score: 0.6424\n",
      "Epoch: 25, Avg Val Loss: 0.7064, Avg Val F1 Score: 0.6208\n",
      "Epoch: 26, Avg Val Loss: 0.7074, Avg Val F1 Score: 0.64\n",
      "Epoch: 27, Avg Val Loss: 0.6992, Avg Val F1 Score: 0.6522\n",
      "Epoch: 28, Avg Val Loss: 0.7035, Avg Val F1 Score: 0.6522\n",
      "Epoch: 29, Avg Val Loss: 0.7102, Avg Val F1 Score: 0.6428\n",
      "Epoch: 30, Avg Val Loss: 0.7113, Avg Val F1 Score: 0.6428\n",
      "Epoch: 31, Avg Val Loss: 0.7194, Avg Val F1 Score: 0.6714\n",
      "✅ Best model saved for Q2\n",
      "Epoch: 32, Avg Val Loss: 0.7102, Avg Val F1 Score: 0.6516\n",
      "Epoch: 33, Avg Val Loss: 0.7185, Avg Val F1 Score: 0.6714\n",
      "Epoch: 34, Avg Val Loss: 0.7115, Avg Val F1 Score: 0.6714\n",
      "Epoch: 35, Avg Val Loss: 0.7061, Avg Val F1 Score: 0.678\n",
      "✅ Best model saved for Q2\n",
      "Epoch: 36, Avg Val Loss: 0.7232, Avg Val F1 Score: 0.678\n",
      "Epoch: 37, Avg Val Loss: 0.7402, Avg Val F1 Score: 0.6516\n",
      "Epoch: 38, Avg Val Loss: 0.7549, Avg Val F1 Score: 0.6516\n",
      "Epoch: 39, Avg Val Loss: 0.7279, Avg Val F1 Score: 0.663\n",
      "Epoch: 40, Avg Val Loss: 0.733, Avg Val F1 Score: 0.6894\n",
      "✅ Best model saved for Q2\n",
      "Epoch: 41, Avg Val Loss: 0.7561, Avg Val F1 Score: 0.6318\n",
      "Epoch: 42, Avg Val Loss: 0.7859, Avg Val F1 Score: 0.6318\n",
      "Epoch: 43, Avg Val Loss: 0.7977, Avg Val F1 Score: 0.6237\n",
      "Epoch: 44, Avg Val Loss: 0.7868, Avg Val F1 Score: 0.6516\n",
      "Epoch: 45, Avg Val Loss: 0.7987, Avg Val F1 Score: 0.6433\n",
      "Epoch: 46, Avg Val Loss: 0.8256, Avg Val F1 Score: 0.6318\n",
      "Epoch: 47, Avg Val Loss: 0.8419, Avg Val F1 Score: 0.6199\n",
      "Epoch: 48, Avg Val Loss: 0.842, Avg Val F1 Score: 0.6107\n",
      "Epoch: 49, Avg Val Loss: 0.8562, Avg Val F1 Score: 0.642\n",
      "Epoch: 50, Avg Val Loss: 0.906, Avg Val F1 Score: 0.6318\n",
      "Training complete.\n",
      "torch.Size([310, 14, 126]) torch.Size([310])\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.6829, Avg Val F1 Score: 0.5022\n",
      "✅ Best model saved for Q3\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.6796, Avg Val F1 Score: 0.5152\n",
      "✅ Best model saved for Q3\n",
      "Epoch 3: Warm-up LR = 0.000030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j1/l91_nhvj6jddv1qmvcx7cfkc0000gn/T/ipykernel_46496/2878537102.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Avg Val Loss: 0.6773, Avg Val F1 Score: 0.5149\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.679, Avg Val F1 Score: 0.547\n",
      "✅ Best model saved for Q3\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.6811, Avg Val F1 Score: 0.5188\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.6821, Avg Val F1 Score: 0.4833\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.6852, Avg Val F1 Score: 0.4919\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.6858, Avg Val F1 Score: 0.5005\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.6883, Avg Val F1 Score: 0.5188\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.6861, Avg Val F1 Score: 0.5222\n",
      "Epoch: 11, Avg Val Loss: 0.6886, Avg Val F1 Score: 0.4898\n",
      "Epoch: 12, Avg Val Loss: 0.6895, Avg Val F1 Score: 0.4898\n",
      "Epoch: 13, Avg Val Loss: 0.6921, Avg Val F1 Score: 0.4735\n",
      "Epoch: 14, Avg Val Loss: 0.6917, Avg Val F1 Score: 0.4735\n",
      "Epoch: 15, Avg Val Loss: 0.6905, Avg Val F1 Score: 0.4942\n",
      "Epoch: 16, Avg Val Loss: 0.6918, Avg Val F1 Score: 0.4942\n",
      "Epoch: 17, Avg Val Loss: 0.6997, Avg Val F1 Score: 0.4942\n",
      "Epoch: 18, Avg Val Loss: 0.6981, Avg Val F1 Score: 0.4942\n",
      "Epoch: 19, Avg Val Loss: 0.6961, Avg Val F1 Score: 0.4942\n",
      "Early stopping triggered.\n",
      "torch.Size([310, 14, 126]) torch.Size([310])\n",
      "Epoch 1: Warm-up LR = 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j1/l91_nhvj6jddv1qmvcx7cfkc0000gn/T/ipykernel_46496/2878537102.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Avg Val Loss: 0.7402, Avg Val F1 Score: 0.548\n",
      "✅ Best model saved for S2\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.7306, Avg Val F1 Score: 0.6101\n",
      "✅ Best model saved for S2\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.7197, Avg Val F1 Score: 0.6037\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.7073, Avg Val F1 Score: 0.6073\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.6962, Avg Val F1 Score: 0.6119\n",
      "✅ Best model saved for S2\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.6878, Avg Val F1 Score: 0.5271\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.6795, Avg Val F1 Score: 0.4429\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.6728, Avg Val F1 Score: 0.4429\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.6627, Avg Val F1 Score: 0.4429\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.6502, Avg Val F1 Score: 0.4429\n",
      "Epoch: 11, Avg Val Loss: 0.6403, Avg Val F1 Score: 0.4429\n",
      "Epoch: 12, Avg Val Loss: 0.6309, Avg Val F1 Score: 0.4429\n",
      "Epoch: 13, Avg Val Loss: 0.6295, Avg Val F1 Score: 0.4429\n",
      "Epoch: 14, Avg Val Loss: 0.622, Avg Val F1 Score: 0.5197\n",
      "Epoch: 15, Avg Val Loss: 0.6129, Avg Val F1 Score: 0.5197\n",
      "Epoch: 16, Avg Val Loss: 0.6063, Avg Val F1 Score: 0.6128\n",
      "✅ Best model saved for S2\n",
      "Epoch: 17, Avg Val Loss: 0.6024, Avg Val F1 Score: 0.5947\n",
      "Epoch: 18, Avg Val Loss: 0.6012, Avg Val F1 Score: 0.6432\n",
      "✅ Best model saved for S2\n",
      "Epoch: 19, Avg Val Loss: 0.5994, Avg Val F1 Score: 0.6153\n",
      "Epoch: 20, Avg Val Loss: 0.5996, Avg Val F1 Score: 0.6153\n",
      "Epoch: 21, Avg Val Loss: 0.5985, Avg Val F1 Score: 0.6432\n",
      "Epoch: 22, Avg Val Loss: 0.6024, Avg Val F1 Score: 0.6234\n",
      "Epoch: 23, Avg Val Loss: 0.6037, Avg Val F1 Score: 0.6234\n",
      "Epoch: 24, Avg Val Loss: 0.6024, Avg Val F1 Score: 0.6234\n",
      "Epoch: 25, Avg Val Loss: 0.6028, Avg Val F1 Score: 0.6234\n",
      "Epoch: 26, Avg Val Loss: 0.6053, Avg Val F1 Score: 0.6234\n",
      "Epoch: 27, Avg Val Loss: 0.6113, Avg Val F1 Score: 0.6458\n",
      "✅ Best model saved for S2\n",
      "Epoch: 28, Avg Val Loss: 0.6108, Avg Val F1 Score: 0.6458\n",
      "Epoch: 29, Avg Val Loss: 0.6125, Avg Val F1 Score: 0.6186\n",
      "Epoch: 30, Avg Val Loss: 0.6134, Avg Val F1 Score: 0.6186\n",
      "Epoch: 31, Avg Val Loss: 0.6206, Avg Val F1 Score: 0.5789\n",
      "Epoch: 32, Avg Val Loss: 0.6232, Avg Val F1 Score: 0.5996\n",
      "Epoch: 33, Avg Val Loss: 0.6222, Avg Val F1 Score: 0.5996\n",
      "Epoch: 34, Avg Val Loss: 0.6264, Avg Val F1 Score: 0.5899\n",
      "Epoch: 35, Avg Val Loss: 0.6265, Avg Val F1 Score: 0.5789\n",
      "Epoch: 36, Avg Val Loss: 0.6239, Avg Val F1 Score: 0.5608\n",
      "Epoch: 37, Avg Val Loss: 0.623, Avg Val F1 Score: 0.552\n",
      "Epoch: 38, Avg Val Loss: 0.6368, Avg Val F1 Score: 0.552\n",
      "Epoch: 39, Avg Val Loss: 0.6425, Avg Val F1 Score: 0.5805\n",
      "Epoch: 40, Avg Val Loss: 0.6392, Avg Val F1 Score: 0.5805\n",
      "Epoch: 41, Avg Val Loss: 0.6328, Avg Val F1 Score: 0.5805\n",
      "Epoch: 42, Avg Val Loss: 0.621, Avg Val F1 Score: 0.6159\n",
      "Early stopping triggered.\n",
      "torch.Size([310, 14, 126]) torch.Size([310])\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.7932, Avg Val F1 Score: 0.5601\n",
      "✅ Best model saved for S3\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.778, Avg Val F1 Score: 0.5882\n",
      "✅ Best model saved for S3\n",
      "Epoch 3: Warm-up LR = 0.000030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j1/l91_nhvj6jddv1qmvcx7cfkc0000gn/T/ipykernel_46496/2878537102.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Avg Val Loss: 0.758, Avg Val F1 Score: 0.5203\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.7349, Avg Val F1 Score: 0.529\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.7134, Avg Val F1 Score: 0.5617\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.6891, Avg Val F1 Score: 0.6276\n",
      "✅ Best model saved for S3\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.6696, Avg Val F1 Score: 0.597\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.6532, Avg Val F1 Score: 0.5622\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.6383, Avg Val F1 Score: 0.5289\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.6271, Avg Val F1 Score: 0.4949\n",
      "Epoch: 11, Avg Val Loss: 0.621, Avg Val F1 Score: 0.4898\n",
      "Epoch: 12, Avg Val Loss: 0.617, Avg Val F1 Score: 0.5534\n",
      "Epoch: 13, Avg Val Loss: 0.6188, Avg Val F1 Score: 0.5868\n",
      "Epoch: 14, Avg Val Loss: 0.6201, Avg Val F1 Score: 0.6493\n",
      "✅ Best model saved for S3\n",
      "Epoch: 15, Avg Val Loss: 0.6218, Avg Val F1 Score: 0.6396\n",
      "Epoch: 16, Avg Val Loss: 0.6253, Avg Val F1 Score: 0.6396\n",
      "Epoch: 17, Avg Val Loss: 0.6297, Avg Val F1 Score: 0.6396\n",
      "Epoch: 18, Avg Val Loss: 0.6331, Avg Val F1 Score: 0.6396\n",
      "Epoch: 19, Avg Val Loss: 0.6361, Avg Val F1 Score: 0.6584\n",
      "✅ Best model saved for S3\n",
      "Epoch: 20, Avg Val Loss: 0.6405, Avg Val F1 Score: 0.6584\n",
      "Epoch: 21, Avg Val Loss: 0.646, Avg Val F1 Score: 0.6584\n",
      "Epoch: 22, Avg Val Loss: 0.6493, Avg Val F1 Score: 0.684\n",
      "✅ Best model saved for S3\n",
      "Epoch: 23, Avg Val Loss: 0.6552, Avg Val F1 Score: 0.647\n",
      "Epoch: 24, Avg Val Loss: 0.6622, Avg Val F1 Score: 0.6602\n",
      "Epoch: 25, Avg Val Loss: 0.6723, Avg Val F1 Score: 0.6263\n",
      "Epoch: 26, Avg Val Loss: 0.6782, Avg Val F1 Score: 0.6512\n",
      "Epoch: 27, Avg Val Loss: 0.6876, Avg Val F1 Score: 0.6163\n",
      "Epoch: 28, Avg Val Loss: 0.695, Avg Val F1 Score: 0.6163\n",
      "Epoch: 29, Avg Val Loss: 0.7023, Avg Val F1 Score: 0.6404\n",
      "Epoch: 30, Avg Val Loss: 0.7149, Avg Val F1 Score: 0.6314\n",
      "Epoch: 31, Avg Val Loss: 0.7221, Avg Val F1 Score: 0.6073\n",
      "Epoch: 32, Avg Val Loss: 0.7205, Avg Val F1 Score: 0.6073\n",
      "Epoch: 33, Avg Val Loss: 0.7311, Avg Val F1 Score: 0.6073\n",
      "Epoch: 34, Avg Val Loss: 0.7264, Avg Val F1 Score: 0.6314\n",
      "Epoch: 35, Avg Val Loss: 0.7345, Avg Val F1 Score: 0.6314\n",
      "Epoch: 36, Avg Val Loss: 0.7425, Avg Val F1 Score: 0.6314\n",
      "Epoch: 37, Avg Val Loss: 0.7499, Avg Val F1 Score: 0.6314\n",
      "Early stopping triggered.\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 1.1153, Avg Val F1 Score: 0.3537\n",
      "✅ Best model saved for S1\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 1.104, Avg Val F1 Score: 0.3753\n",
      "✅ Best model saved for S1\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 1.0908, Avg Val F1 Score: 0.4119\n",
      "✅ Best model saved for S1\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 1.0758, Avg Val F1 Score: 0.402\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 1.0602, Avg Val F1 Score: 0.4231\n",
      "✅ Best model saved for S1\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 1.0459, Avg Val F1 Score: 0.3872\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 1.0344, Avg Val F1 Score: 0.4457\n",
      "✅ Best model saved for S1\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 1.0277, Avg Val F1 Score: 0.4622\n",
      "✅ Best model saved for S1\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 1.0278, Avg Val F1 Score: 0.4546\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 1.0234, Avg Val F1 Score: 0.4127\n",
      "Epoch: 11, Avg Val Loss: 1.0183, Avg Val F1 Score: 0.4548\n",
      "Epoch: 12, Avg Val Loss: 1.0172, Avg Val F1 Score: 0.4654\n",
      "✅ Best model saved for S1\n",
      "Epoch: 13, Avg Val Loss: 1.013, Avg Val F1 Score: 0.4548\n",
      "Epoch: 14, Avg Val Loss: 1.011, Avg Val F1 Score: 0.4548\n",
      "Epoch: 15, Avg Val Loss: 1.0081, Avg Val F1 Score: 0.4361\n",
      "Epoch: 16, Avg Val Loss: 1.0048, Avg Val F1 Score: 0.4304\n",
      "Epoch: 17, Avg Val Loss: 1.0062, Avg Val F1 Score: 0.4304\n",
      "Epoch: 18, Avg Val Loss: 1.0088, Avg Val F1 Score: 0.4221\n",
      "Epoch: 19, Avg Val Loss: 1.0139, Avg Val F1 Score: 0.4221\n",
      "Epoch: 20, Avg Val Loss: 1.0167, Avg Val F1 Score: 0.4221\n",
      "Epoch: 21, Avg Val Loss: 1.0201, Avg Val F1 Score: 0.3763\n",
      "Epoch: 22, Avg Val Loss: 1.0214, Avg Val F1 Score: 0.3781\n",
      "Epoch: 23, Avg Val Loss: 1.0236, Avg Val F1 Score: 0.3719\n",
      "Epoch: 24, Avg Val Loss: 1.0287, Avg Val F1 Score: 0.3829\n",
      "Epoch: 25, Avg Val Loss: 1.0235, Avg Val F1 Score: 0.3997\n",
      "Epoch: 26, Avg Val Loss: 1.0222, Avg Val F1 Score: 0.3701\n",
      "Epoch: 27, Avg Val Loss: 1.0294, Avg Val F1 Score: 0.3719\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "from models.fftformer import *\n",
    "from train import *\n",
    "from prediction import *\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "binary_preds = {}\n",
    "binary_f1 = {}\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "hidden_dim = 32\n",
    "num_heads = 32\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "\n",
    "for col in targets_binary:\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(np.array([[y[col]] for y in Y_seq]), dtype=torch.long).view(-1)\n",
    "    \n",
    "    print(X_train.shape, y_train.shape)\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    model_bin = FFTformerClassifier(input_dim=X_train.shape[-1], num_classes=2, hidden_dim=int(hidden_dim), num_heads=int(num_heads))\n",
    "    binary_f1[col] = train_model(model_bin, dataloader, nn.CrossEntropyLoss(), optim.Adam(model_bin.parameters(), lr=0.0001), col = col, epochs=30)\n",
    "    binary_preds[col] = predict(model_bin, torch.tensor(X_test, dtype=torch.float32), col)\n",
    "    f1_scores.append(binary_f1[col])\n",
    "\n",
    "y_multi_train = torch.tensor(np.array([[y[target_multiclass]] for y in Y_seq]), dtype=torch.long).view(-1)\n",
    "dataset_multi = TensorDataset(X_train, y_multi_train)\n",
    "dataloader_multi = DataLoader(dataset_multi, batch_size=32, shuffle=True)\n",
    "\n",
    "model_multi = FFTformerClassifier(input_dim=X_train.shape[-1], num_classes=3, hidden_dim=int(hidden_dim), num_heads=int(num_heads))\n",
    "multiclass_f1 = train_model(model_multi, dataloader_multi, nn.CrossEntropyLoss(), optim.Adam(model_multi.parameters(), lr=0.0001), col = 'S1', epochs=30)\n",
    "f1_scores.append(multiclass_f1)\n",
    "\n",
    "multiclass_pred = predict(model_multi, torch.tensor(X_test, dtype=torch.float32), 'S1')\n",
    "avg_f1 = sum(f1_scores) / len(f1_scores)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 제출 파일 생성 완료: submission_hidden_32_head_32_f1_inf).csv\n"
     ]
    }
   ],
   "source": [
    "generate_submission(sample_submission, binary_preds, multiclass_pred, f'submission_hidden_{hidden_dim}_head_{num_heads}_f1_{round(avg_f1, 4)}).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
