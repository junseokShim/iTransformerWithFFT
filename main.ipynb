{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('dataset/merged_df.csv')\n",
    "metrics_train = pd.read_csv('dataset/ch2025_data_items/ch2025_metrics_train.csv')\n",
    "sample_submission = pd.read_csv('dataset/ch2025_submission_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = merged_df.subject_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_data(metrics_train, merged_df):\n",
    "    metrics_train['lifelog_date'] = pd.to_datetime(metrics_train['lifelog_date']).dt.date\n",
    "    merged_df['date'] = pd.to_datetime(merged_df['date']).dt.date\n",
    "\n",
    "    metrics_train = metrics_train.rename(columns={'lifelog_date': 'date'})\n",
    "\n",
    "    train_df = pd.merge(metrics_train, merged_df, on=['subject_id', 'date'], how='inner')\n",
    "\n",
    "    merged_keys = merged_df[['subject_id', 'date']]\n",
    "    train_keys = metrics_train[['subject_id', 'date']]\n",
    "    test_keys = pd.merge(merged_keys, train_keys, on=['subject_id', 'date'], how='left', indicator=True)\n",
    "    test_keys = test_keys[test_keys['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "    test_df = pd.merge(test_keys, merged_df, on=['subject_id', 'date'], how='left')\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def generate_submission(sample_submission, binary_preds, multiclass_pred, filename):\n",
    "    sample_submission['lifelog_date'] = pd.to_datetime(sample_submission['lifelog_date']).dt.date\n",
    "    submission_final = sample_submission[['subject_id', 'sleep_date', 'lifelog_date']].copy()\n",
    "    submission_final['ID'] = submission_final['subject_id'] + '_' + submission_final['lifelog_date'].astype(str)\n",
    "\n",
    "    submission_final['S1'] = multiclass_pred\n",
    "    for col in binary_preds:\n",
    "        submission_final[col] = binary_preds[col].astype(int)\n",
    "\n",
    "    submission_final = submission_final[['subject_id', 'sleep_date', 'lifelog_date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']]\n",
    "    submission_final.to_csv(filename, index=False)\n",
    "    print(f\"✅ 제출 파일 생성 완료: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = prepare_train_test_data(metrics_train, merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_binary = ['Q1', 'Q2', 'Q3', 'S2', 'S3']\n",
    "target_multiclass = 'S1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=['sleep_date', 'date'] + targets_binary + [target_multiclass]).fillna(0)\n",
    "\n",
    "Y = train_df[['subject_id'] + targets_binary + [target_multiclass]].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 14, 126) (310, 6)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 14\n",
    "\n",
    "X_seq = []\n",
    "Y_seq = []\n",
    "\n",
    "for user in users:\n",
    "    for i in range(X[X.subject_id==user].shape[0]-seq_len):\n",
    "        X_seq.append(X[X.subject_id==user].iloc[i:i+seq_len, 1:].to_numpy())\n",
    "        Y_seq.append(Y[Y.subject_id==user].iloc[i+seq_len, 1:])\n",
    "    \n",
    "X_seq = np.array(X_seq)\n",
    "#Y_seq = np.array(Y_seq)\n",
    "\n",
    "print(X_seq.shape, np.array(Y_seq).shape)\n",
    "\n",
    "X_seq_len = X_seq.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "test_X_seq = []\n",
    "\n",
    "for user in users:\n",
    "    for date in sample_submission[sample_submission.subject_id == user].lifelog_date:\n",
    "        c_index = merged_df[(merged_df['subject_id'] == user) & (merged_df['date'] == dt.datetime.strptime(date, \"%Y-%m-%d\").date())].index\n",
    "        test_X_seq.append(merged_df.iloc[c_index[0]-seq_len:c_index[0], :].drop(columns=['subject_id', 'date']).fillna(0).to_numpy())\n",
    "        \n",
    "test_X_seq = np.array(test_X_seq)\n",
    "test_X_seq_len = test_X_seq.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 14, 126)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "# Example: Check the shape of X_seq\n",
    "print(X_seq.shape)\n",
    "\n",
    "# If X_seq is 3D, reshape it to 2D\n",
    "if len(X_seq.shape) == 3:\n",
    "    X_seq = X_seq.reshape(-1, X_seq.shape[-1])\n",
    "    test_X_seq = test_X_seq.reshape(-1, test_X_seq.shape[-1])\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_seq)\n",
    "X_train = X_train_scaled.reshape(X_seq_len, seq_len, 126)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "\n",
    "X_test_scaled = scaler.transform(test_X_seq)\n",
    "X_test = X_test_scaled.reshape(test_X_seq_len, seq_len, 126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([310, 14, 126]) torch.Size([310])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j1/l91_nhvj6jddv1qmvcx7cfkc0000gn/T/ipykernel_49391/1706277648.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.6934, Avg Val F1 Score: 0.5643\n",
      "✅ Best model saved for Q1\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.6939, Avg Val F1 Score: 0.5531\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.7025, Avg Val F1 Score: 0.4842\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.7322, Avg Val F1 Score: 0.4524\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.7587, Avg Val F1 Score: 0.434\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.7817, Avg Val F1 Score: 0.4995\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.7636, Avg Val F1 Score: 0.5673\n",
      "✅ Best model saved for Q1\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.7606, Avg Val F1 Score: 0.5643\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.7685, Avg Val F1 Score: 0.5649\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.7729, Avg Val F1 Score: 0.7173\n",
      "✅ Best model saved for Q1\n",
      "Epoch: 11, Avg Val Loss: 0.7922, Avg Val F1 Score: 0.5789\n",
      "Epoch: 12, Avg Val Loss: 0.8039, Avg Val F1 Score: 0.6228\n",
      "Epoch: 13, Avg Val Loss: 0.7915, Avg Val F1 Score: 0.6327\n",
      "Epoch: 14, Avg Val Loss: 0.7839, Avg Val F1 Score: 0.6764\n",
      "Epoch: 15, Avg Val Loss: 0.8303, Avg Val F1 Score: 0.6643\n",
      "Epoch: 16, Avg Val Loss: 0.88, Avg Val F1 Score: 0.6799\n",
      "Epoch: 17, Avg Val Loss: 0.8271, Avg Val F1 Score: 0.6764\n",
      "Epoch: 18, Avg Val Loss: 0.8158, Avg Val F1 Score: 0.6697\n",
      "Epoch: 19, Avg Val Loss: 0.8537, Avg Val F1 Score: 0.678\n",
      "Epoch: 20, Avg Val Loss: 0.8213, Avg Val F1 Score: 0.6615\n",
      "Epoch: 21, Avg Val Loss: 0.844, Avg Val F1 Score: 0.712\n",
      "Epoch: 22, Avg Val Loss: 0.9546, Avg Val F1 Score: 0.6423\n",
      "Epoch: 23, Avg Val Loss: 1.0424, Avg Val F1 Score: 0.6942\n",
      "Epoch: 24, Avg Val Loss: 0.9533, Avg Val F1 Score: 0.6861\n",
      "Epoch: 25, Avg Val Loss: 0.9245, Avg Val F1 Score: 0.6963\n",
      "Early stopping triggered.\n",
      "torch.Size([310, 14, 126]) torch.Size([310])\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.7205, Avg Val F1 Score: 0.4542\n",
      "✅ Best model saved for Q2\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.6804, Avg Val F1 Score: 0.5182\n",
      "✅ Best model saved for Q2\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.6581, Avg Val F1 Score: 0.5497\n",
      "✅ Best model saved for Q2\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.638, Avg Val F1 Score: 0.6219\n",
      "✅ Best model saved for Q2\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.635, Avg Val F1 Score: 0.675\n",
      "✅ Best model saved for Q2\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.6584, Avg Val F1 Score: 0.5986\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.7031, Avg Val F1 Score: 0.5931\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.7167, Avg Val F1 Score: 0.6765\n",
      "✅ Best model saved for Q2\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.7143, Avg Val F1 Score: 0.6626\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.7777, Avg Val F1 Score: 0.6436\n",
      "Epoch: 11, Avg Val Loss: 0.7957, Avg Val F1 Score: 0.6359\n",
      "Epoch: 12, Avg Val Loss: 0.8491, Avg Val F1 Score: 0.5989\n",
      "Epoch: 13, Avg Val Loss: 0.8496, Avg Val F1 Score: 0.6359\n",
      "Epoch: 14, Avg Val Loss: 0.944, Avg Val F1 Score: 0.6194\n",
      "Epoch: 15, Avg Val Loss: 0.9838, Avg Val F1 Score: 0.6141\n",
      "Epoch: 16, Avg Val Loss: 0.96, Avg Val F1 Score: 0.621\n",
      "Epoch: 17, Avg Val Loss: 0.98, Avg Val F1 Score: 0.6359\n",
      "Epoch: 18, Avg Val Loss: 1.0719, Avg Val F1 Score: 0.657\n",
      "Epoch: 19, Avg Val Loss: 1.0833, Avg Val F1 Score: 0.6671\n",
      "Epoch: 20, Avg Val Loss: 1.0835, Avg Val F1 Score: 0.6404\n",
      "Epoch: 21, Avg Val Loss: 1.1174, Avg Val F1 Score: 0.6957\n",
      "✅ Best model saved for Q2\n",
      "Epoch: 22, Avg Val Loss: 1.1122, Avg Val F1 Score: 0.6931\n",
      "Epoch: 23, Avg Val Loss: 1.0881, Avg Val F1 Score: 0.5751\n",
      "Epoch: 24, Avg Val Loss: 1.0244, Avg Val F1 Score: 0.6359\n",
      "Epoch: 25, Avg Val Loss: 1.1137, Avg Val F1 Score: 0.6101\n",
      "Epoch: 26, Avg Val Loss: 1.0274, Avg Val F1 Score: 0.6588\n",
      "Epoch: 27, Avg Val Loss: 1.2823, Avg Val F1 Score: 0.5826\n",
      "Epoch: 28, Avg Val Loss: 1.2436, Avg Val F1 Score: 0.657\n",
      "Epoch: 29, Avg Val Loss: 1.3591, Avg Val F1 Score: 0.6836\n",
      "Epoch: 30, Avg Val Loss: 1.4642, Avg Val F1 Score: 0.6447\n",
      "Training complete.\n",
      "torch.Size([310, 14, 126]) torch.Size([310])\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.7813, Avg Val F1 Score: 0.4931\n",
      "✅ Best model saved for Q3\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.7686, Avg Val F1 Score: 0.4697\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.765, Avg Val F1 Score: 0.4841\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.7329, Avg Val F1 Score: 0.5667\n",
      "✅ Best model saved for Q3\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.7076, Avg Val F1 Score: 0.6289\n",
      "✅ Best model saved for Q3\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.689, Avg Val F1 Score: 0.5898\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.6591, Avg Val F1 Score: 0.5748\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.6861, Avg Val F1 Score: 0.6209\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.7174, Avg Val F1 Score: 0.5977\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.7374, Avg Val F1 Score: 0.5701\n",
      "Epoch: 11, Avg Val Loss: 0.688, Avg Val F1 Score: 0.5956\n",
      "Epoch: 12, Avg Val Loss: 0.6534, Avg Val F1 Score: 0.6823\n",
      "✅ Best model saved for Q3\n",
      "Epoch: 13, Avg Val Loss: 0.6917, Avg Val F1 Score: 0.6263\n",
      "Epoch: 14, Avg Val Loss: 0.6507, Avg Val F1 Score: 0.7175\n",
      "✅ Best model saved for Q3\n",
      "Epoch: 15, Avg Val Loss: 0.656, Avg Val F1 Score: 0.6616\n",
      "Epoch: 16, Avg Val Loss: 0.6895, Avg Val F1 Score: 0.6507\n",
      "Epoch: 17, Avg Val Loss: 0.626, Avg Val F1 Score: 0.6923\n",
      "Epoch: 18, Avg Val Loss: 0.6864, Avg Val F1 Score: 0.6264\n",
      "Epoch: 19, Avg Val Loss: 0.7044, Avg Val F1 Score: 0.6508\n",
      "Epoch: 20, Avg Val Loss: 0.6433, Avg Val F1 Score: 0.661\n",
      "Epoch: 21, Avg Val Loss: 0.6792, Avg Val F1 Score: 0.6616\n",
      "Epoch: 22, Avg Val Loss: 0.6192, Avg Val F1 Score: 0.6666\n",
      "Epoch: 23, Avg Val Loss: 0.6265, Avg Val F1 Score: 0.708\n",
      "Epoch: 24, Avg Val Loss: 0.7738, Avg Val F1 Score: 0.624\n",
      "Epoch: 25, Avg Val Loss: 0.6749, Avg Val F1 Score: 0.655\n",
      "Epoch: 26, Avg Val Loss: 0.7286, Avg Val F1 Score: 0.613\n",
      "Epoch: 27, Avg Val Loss: 0.7229, Avg Val F1 Score: 0.6297\n",
      "Epoch: 28, Avg Val Loss: 0.7589, Avg Val F1 Score: 0.679\n",
      "Epoch: 29, Avg Val Loss: 0.7269, Avg Val F1 Score: 0.6952\n",
      "Early stopping triggered.\n",
      "torch.Size([310, 14, 126]) torch.Size([310])\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.7435, Avg Val F1 Score: 0.5614\n",
      "✅ Best model saved for S2\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.695, Avg Val F1 Score: 0.568\n",
      "✅ Best model saved for S2\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.6543, Avg Val F1 Score: 0.6032\n",
      "✅ Best model saved for S2\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.6052, Avg Val F1 Score: 0.5845\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.5851, Avg Val F1 Score: 0.6332\n",
      "✅ Best model saved for S2\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.5854, Avg Val F1 Score: 0.6332\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.617, Avg Val F1 Score: 0.6332\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.6535, Avg Val F1 Score: 0.6212\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.6814, Avg Val F1 Score: 0.6332\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.7378, Avg Val F1 Score: 0.6212\n",
      "Epoch: 11, Avg Val Loss: 0.741, Avg Val F1 Score: 0.5837\n",
      "Epoch: 12, Avg Val Loss: 0.8085, Avg Val F1 Score: 0.6386\n",
      "✅ Best model saved for S2\n",
      "Epoch: 13, Avg Val Loss: 0.814, Avg Val F1 Score: 0.6494\n",
      "✅ Best model saved for S2\n",
      "Epoch: 14, Avg Val Loss: 0.7858, Avg Val F1 Score: 0.7105\n",
      "✅ Best model saved for S2\n",
      "Epoch: 15, Avg Val Loss: 0.8048, Avg Val F1 Score: 0.6825\n",
      "Epoch: 16, Avg Val Loss: 0.8448, Avg Val F1 Score: 0.6631\n",
      "Epoch: 17, Avg Val Loss: 0.8703, Avg Val F1 Score: 0.6423\n",
      "Epoch: 18, Avg Val Loss: 0.8551, Avg Val F1 Score: 0.6513\n",
      "Epoch: 19, Avg Val Loss: 0.8199, Avg Val F1 Score: 0.6825\n",
      "Epoch: 20, Avg Val Loss: 0.9314, Avg Val F1 Score: 0.6631\n",
      "Epoch: 21, Avg Val Loss: 0.9002, Avg Val F1 Score: 0.6317\n",
      "Epoch: 22, Avg Val Loss: 0.9078, Avg Val F1 Score: 0.6743\n",
      "Epoch: 23, Avg Val Loss: 0.9288, Avg Val F1 Score: 0.6722\n",
      "Epoch: 24, Avg Val Loss: 0.9908, Avg Val F1 Score: 0.6366\n",
      "Epoch: 25, Avg Val Loss: 0.9911, Avg Val F1 Score: 0.6601\n",
      "Epoch: 26, Avg Val Loss: 0.9613, Avg Val F1 Score: 0.5965\n",
      "Epoch: 27, Avg Val Loss: 1.0119, Avg Val F1 Score: 0.6495\n",
      "Epoch: 28, Avg Val Loss: 0.9357, Avg Val F1 Score: 0.6812\n",
      "Epoch: 29, Avg Val Loss: 1.036, Avg Val F1 Score: 0.6802\n",
      "Early stopping triggered.\n",
      "torch.Size([310, 14, 126]) torch.Size([310])\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.7076, Avg Val F1 Score: 0.5401\n",
      "✅ Best model saved for S3\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.594, Avg Val F1 Score: 0.7039\n",
      "✅ Best model saved for S3\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.5552, Avg Val F1 Score: 0.5794\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.4873, Avg Val F1 Score: 0.7224\n",
      "✅ Best model saved for S3\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.4396, Avg Val F1 Score: 0.8191\n",
      "✅ Best model saved for S3\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.4092, Avg Val F1 Score: 0.7999\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.3925, Avg Val F1 Score: 0.7999\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.3915, Avg Val F1 Score: 0.7999\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.4018, Avg Val F1 Score: 0.7664\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.4137, Avg Val F1 Score: 0.7153\n",
      "Epoch: 11, Avg Val Loss: 0.4272, Avg Val F1 Score: 0.716\n",
      "Epoch: 12, Avg Val Loss: 0.4077, Avg Val F1 Score: 0.7568\n",
      "Epoch: 13, Avg Val Loss: 0.4137, Avg Val F1 Score: 0.7647\n",
      "Epoch: 14, Avg Val Loss: 0.4215, Avg Val F1 Score: 0.7936\n",
      "Epoch: 15, Avg Val Loss: 0.4575, Avg Val F1 Score: 0.7153\n",
      "Epoch: 16, Avg Val Loss: 0.4418, Avg Val F1 Score: 0.7746\n",
      "Epoch: 17, Avg Val Loss: 0.4694, Avg Val F1 Score: 0.7261\n",
      "Epoch: 18, Avg Val Loss: 0.4568, Avg Val F1 Score: 0.8111\n",
      "Epoch: 19, Avg Val Loss: 0.476, Avg Val F1 Score: 0.7564\n",
      "Epoch: 20, Avg Val Loss: 0.4764, Avg Val F1 Score: 0.7504\n",
      "Early stopping triggered.\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 1.1181, Avg Val F1 Score: 0.3457\n",
      "✅ Best model saved for S1\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 1.0544, Avg Val F1 Score: 0.3428\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 1.011, Avg Val F1 Score: 0.401\n",
      "✅ Best model saved for S1\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 1.0192, Avg Val F1 Score: 0.4115\n",
      "✅ Best model saved for S1\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 1.0392, Avg Val F1 Score: 0.4708\n",
      "✅ Best model saved for S1\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 1.0594, Avg Val F1 Score: 0.5788\n",
      "✅ Best model saved for S1\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 1.0892, Avg Val F1 Score: 0.5578\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 1.0983, Avg Val F1 Score: 0.5656\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 1.0831, Avg Val F1 Score: 0.5384\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 1.0795, Avg Val F1 Score: 0.5397\n",
      "Epoch: 11, Avg Val Loss: 1.1321, Avg Val F1 Score: 0.5656\n",
      "Epoch: 12, Avg Val Loss: 1.1339, Avg Val F1 Score: 0.5792\n",
      "✅ Best model saved for S1\n",
      "Epoch: 13, Avg Val Loss: 1.2057, Avg Val F1 Score: 0.5588\n",
      "Epoch: 14, Avg Val Loss: 1.1424, Avg Val F1 Score: 0.5117\n",
      "Epoch: 15, Avg Val Loss: 1.1063, Avg Val F1 Score: 0.5131\n",
      "Epoch: 16, Avg Val Loss: 1.1533, Avg Val F1 Score: 0.5838\n",
      "✅ Best model saved for S1\n",
      "Epoch: 17, Avg Val Loss: 1.1574, Avg Val F1 Score: 0.5851\n",
      "✅ Best model saved for S1\n",
      "Epoch: 18, Avg Val Loss: 1.1651, Avg Val F1 Score: 0.5736\n",
      "Epoch: 19, Avg Val Loss: 1.1265, Avg Val F1 Score: 0.6379\n",
      "✅ Best model saved for S1\n",
      "Epoch: 20, Avg Val Loss: 1.1512, Avg Val F1 Score: 0.6513\n",
      "✅ Best model saved for S1\n",
      "Epoch: 21, Avg Val Loss: 1.2302, Avg Val F1 Score: 0.5828\n",
      "Epoch: 22, Avg Val Loss: 1.2334, Avg Val F1 Score: 0.5889\n",
      "Epoch: 23, Avg Val Loss: 1.2444, Avg Val F1 Score: 0.5614\n",
      "Epoch: 24, Avg Val Loss: 1.2877, Avg Val F1 Score: 0.5574\n",
      "Epoch: 25, Avg Val Loss: 1.2516, Avg Val F1 Score: 0.5805\n",
      "Epoch: 26, Avg Val Loss: 1.2419, Avg Val F1 Score: 0.5754\n",
      "Epoch: 27, Avg Val Loss: 1.2181, Avg Val F1 Score: 0.5775\n",
      "Epoch: 28, Avg Val Loss: 1.2545, Avg Val F1 Score: 0.5942\n",
      "Epoch: 29, Avg Val Loss: 1.3337, Avg Val F1 Score: 0.5894\n",
      "Epoch: 30, Avg Val Loss: 1.2458, Avg Val F1 Score: 0.5902\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "from models.fftformer import *\n",
    "from train import *\n",
    "from prediction import *\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "seed_value = 42\n",
    "\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "binary_preds = {}\n",
    "binary_f1 = {}\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "hidden_dim = 128\n",
    "num_heads = 64\n",
    "n_layers = 12\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "\n",
    "for col in targets_binary:\n",
    "    y_train = torch.tensor(np.array([[y[col]] for y in Y_seq]), dtype=torch.long).view(-1)\n",
    "    \n",
    "    print(X_train.shape, y_train.shape)\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    model_bin = FFTformerClassifier(input_dim=X_train.shape[-1], num_classes=2, hidden_dim=int(hidden_dim), num_heads=int(num_heads))\n",
    "    binary_f1[col] = train_model(model_bin, dataloader, nn.CrossEntropyLoss(), optim.Adam(model_bin.parameters(), lr=0.0001), col = col, epochs=30)\n",
    "    binary_preds[col] = predict(model_bin, torch.tensor(X_test, dtype=torch.float32), col)\n",
    "    f1_scores.append(binary_f1[col])\n",
    "\n",
    "y_multi_train = torch.tensor(np.array([[y[target_multiclass]] for y in Y_seq]), dtype=torch.long).view(-1)\n",
    "dataset_multi = TensorDataset(X_train, y_multi_train)\n",
    "dataloader_multi = DataLoader(dataset_multi, batch_size=32, shuffle=True)\n",
    "\n",
    "model_multi = FFTformerClassifier(input_dim=X_train.shape[-1], num_classes=3, hidden_dim=int(hidden_dim), num_heads=int(num_heads))\n",
    "multiclass_f1 = train_model(model_multi, dataloader_multi, nn.CrossEntropyLoss(), optim.Adam(model_multi.parameters(), lr=0.0001), col = 'S1', epochs=30)\n",
    "f1_scores.append(multiclass_f1)\n",
    "\n",
    "multiclass_pred = predict(model_multi, torch.tensor(X_test, dtype=torch.float32), 'S1')\n",
    "avg_f1 = sum(f1_scores) / len(f1_scores)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 제출 파일 생성 완료: submission_fftformer_model_hidden_128_head_64_num_layer_12_seq_14.csv\n"
     ]
    }
   ],
   "source": [
    "generate_submission(sample_submission, binary_preds, multiclass_pred, f'submission_fftformer_model_hidden_{hidden_dim}_head_{num_heads}_num_layer_12_seq_14.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
