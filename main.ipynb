{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('dataset/merged_df.csv')\n",
    "metrics_train = pd.read_csv('dataset/ch2025_data_items/ch2025_metrics_train.csv')\n",
    "sample_submission = pd.read_csv('dataset/ch2025_submission_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = merged_df.subject_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_data(metrics_train, merged_df):\n",
    "    metrics_train['lifelog_date'] = pd.to_datetime(metrics_train['lifelog_date']).dt.date\n",
    "    merged_df['date'] = pd.to_datetime(merged_df['date']).dt.date\n",
    "\n",
    "    metrics_train = metrics_train.rename(columns={'lifelog_date': 'date'})\n",
    "\n",
    "    train_df = pd.merge(metrics_train, merged_df, on=['subject_id', 'date'], how='inner')\n",
    "\n",
    "    merged_keys = merged_df[['subject_id', 'date']]\n",
    "    train_keys = metrics_train[['subject_id', 'date']]\n",
    "    test_keys = pd.merge(merged_keys, train_keys, on=['subject_id', 'date'], how='left', indicator=True)\n",
    "    test_keys = test_keys[test_keys['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "    test_df = pd.merge(test_keys, merged_df, on=['subject_id', 'date'], how='left')\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def generate_submission(sample_submission, binary_preds, multiclass_pred, filename):\n",
    "    sample_submission['lifelog_date'] = pd.to_datetime(sample_submission['lifelog_date']).dt.date\n",
    "    submission_final = sample_submission[['subject_id', 'sleep_date', 'lifelog_date']].copy()\n",
    "    submission_final['ID'] = submission_final['subject_id'] + '_' + submission_final['lifelog_date'].astype(str)\n",
    "\n",
    "    submission_final['S1'] = multiclass_pred\n",
    "    for col in binary_preds:\n",
    "        submission_final[col] = binary_preds[col].astype(int)\n",
    "\n",
    "    submission_final = submission_final[['subject_id', 'sleep_date', 'lifelog_date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']]\n",
    "    submission_final.to_csv(filename, index=False)\n",
    "    print(f\"✅ 제출 파일 생성 완료: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = prepare_train_test_data(metrics_train, merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_binary = ['Q1', 'Q2', 'Q3', 'S2', 'S3']\n",
    "target_multiclass = 'S1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=['sleep_date', 'date'] + targets_binary + [target_multiclass]).fillna(0)\n",
    "\n",
    "Y = train_df[['subject_id'] + targets_binary + [target_multiclass]].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 14, 126) (310, 6)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 14\n",
    "\n",
    "X_seq = []\n",
    "Y_seq = []\n",
    "\n",
    "for user in users:\n",
    "    for i in range(X[X.subject_id==user].shape[0]-seq_len):\n",
    "        X_seq.append(X[X.subject_id==user].iloc[i:i+seq_len, 1:].to_numpy())\n",
    "        Y_seq.append(Y[Y.subject_id==user].iloc[i+seq_len, 1:])\n",
    "    \n",
    "X_seq = np.array(X_seq)\n",
    "#Y_seq = np.array(Y_seq)\n",
    "\n",
    "print(X_seq.shape, np.array(Y_seq).shape)\n",
    "\n",
    "X_seq_len = X_seq.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "test_X_seq = []\n",
    "\n",
    "for user in users:\n",
    "    for date in sample_submission[sample_submission.subject_id == user].lifelog_date:\n",
    "        c_index = merged_df[(merged_df['subject_id'] == user) & (merged_df['date'] == dt.datetime.strptime(date, \"%Y-%m-%d\").date())].index\n",
    "        test_X_seq.append(merged_df.iloc[c_index[0]-seq_len:c_index[0], :].drop(columns=['subject_id', 'date']).fillna(0).to_numpy())\n",
    "        \n",
    "test_X_seq = np.array(test_X_seq)\n",
    "test_X_seq_len = test_X_seq.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 14, 126)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "# Example: Check the shape of X_seq\n",
    "print(X_seq.shape)\n",
    "\n",
    "# If X_seq is 3D, reshape it to 2D\n",
    "if len(X_seq.shape) == 3:\n",
    "    X_seq = X_seq.reshape(-1, X_seq.shape[-1])\n",
    "    test_X_seq = test_X_seq.reshape(-1, test_X_seq.shape[-1])\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_seq)\n",
    "X_train = X_train_scaled.reshape(X_seq_len, seq_len, 126)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "\n",
    "X_test_scaled = scaler.transform(test_X_seq)\n",
    "X_test = X_test_scaled.reshape(test_X_seq_len, seq_len, 126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([310, 14, 126]) torch.Size([310])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j1/l91_nhvj6jddv1qmvcx7cfkc0000gn/T/ipykernel_47518/3281406505.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.6958, Avg Val F1 Score: 0.3695\n",
      "✅ Best model saved for Q1\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.6959, Avg Val F1 Score: 0.3695\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.6954, Avg Val F1 Score: 0.3893\n",
      "✅ Best model saved for Q1\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.6954, Avg Val F1 Score: 0.3981\n",
      "✅ Best model saved for Q1\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.6955, Avg Val F1 Score: 0.4151\n",
      "✅ Best model saved for Q1\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.6948, Avg Val F1 Score: 0.4506\n",
      "✅ Best model saved for Q1\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.6943, Avg Val F1 Score: 0.5167\n",
      "✅ Best model saved for Q1\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.6938, Avg Val F1 Score: 0.5024\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.6952, Avg Val F1 Score: 0.5214\n",
      "✅ Best model saved for Q1\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.697, Avg Val F1 Score: 0.4851\n",
      "Epoch: 11, Avg Val Loss: 0.698, Avg Val F1 Score: 0.4823\n",
      "Epoch: 12, Avg Val Loss: 0.6945, Avg Val F1 Score: 0.4794\n",
      "Epoch: 13, Avg Val Loss: 0.6921, Avg Val F1 Score: 0.486\n",
      "Epoch: 14, Avg Val Loss: 0.6916, Avg Val F1 Score: 0.5044\n",
      "Epoch: 15, Avg Val Loss: 0.6936, Avg Val F1 Score: 0.5222\n",
      "✅ Best model saved for Q1\n",
      "Epoch: 16, Avg Val Loss: 0.6994, Avg Val F1 Score: 0.4791\n",
      "Epoch: 17, Avg Val Loss: 0.7042, Avg Val F1 Score: 0.4624\n",
      "Epoch: 18, Avg Val Loss: 0.7143, Avg Val F1 Score: 0.4744\n",
      "Epoch: 19, Avg Val Loss: 0.7169, Avg Val F1 Score: 0.5136\n",
      "Epoch: 20, Avg Val Loss: 0.7252, Avg Val F1 Score: 0.5136\n",
      "Epoch: 21, Avg Val Loss: 0.7254, Avg Val F1 Score: 0.551\n",
      "✅ Best model saved for Q1\n",
      "Epoch: 22, Avg Val Loss: 0.7358, Avg Val F1 Score: 0.551\n",
      "Epoch: 23, Avg Val Loss: 0.747, Avg Val F1 Score: 0.562\n",
      "✅ Best model saved for Q1\n",
      "Epoch: 24, Avg Val Loss: 0.7614, Avg Val F1 Score: 0.556\n",
      "Epoch: 25, Avg Val Loss: 0.7627, Avg Val F1 Score: 0.551\n",
      "Epoch: 26, Avg Val Loss: 0.7607, Avg Val F1 Score: 0.562\n",
      "Epoch: 27, Avg Val Loss: 0.7799, Avg Val F1 Score: 0.5275\n",
      "Epoch: 28, Avg Val Loss: 0.7924, Avg Val F1 Score: 0.5593\n",
      "Epoch: 29, Avg Val Loss: 0.7783, Avg Val F1 Score: 0.5757\n",
      "✅ Best model saved for Q1\n",
      "Epoch: 30, Avg Val Loss: 0.7936, Avg Val F1 Score: 0.542\n",
      "Training complete.\n",
      "torch.Size([310, 14, 126]) torch.Size([310])\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.7035, Avg Val F1 Score: 0.3625\n",
      "✅ Best model saved for Q2\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.7024, Avg Val F1 Score: 0.4279\n",
      "✅ Best model saved for Q2\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.7016, Avg Val F1 Score: 0.4177\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.7003, Avg Val F1 Score: 0.3718\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.6996, Avg Val F1 Score: 0.3498\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.699, Avg Val F1 Score: 0.3062\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.6988, Avg Val F1 Score: 0.3209\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.6994, Avg Val F1 Score: 0.3209\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.6997, Avg Val F1 Score: 0.3209\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.7, Avg Val F1 Score: 0.3209\n",
      "Epoch: 11, Avg Val Loss: 0.699, Avg Val F1 Score: 0.3209\n",
      "Epoch: 12, Avg Val Loss: 0.6995, Avg Val F1 Score: 0.3209\n",
      "Epoch: 13, Avg Val Loss: 0.6991, Avg Val F1 Score: 0.3209\n",
      "Epoch: 14, Avg Val Loss: 0.7001, Avg Val F1 Score: 0.3209\n",
      "Epoch: 15, Avg Val Loss: 0.6934, Avg Val F1 Score: 0.3209\n",
      "Epoch: 16, Avg Val Loss: 0.6907, Avg Val F1 Score: 0.4254\n",
      "Epoch: 17, Avg Val Loss: 0.6873, Avg Val F1 Score: 0.5679\n",
      "✅ Best model saved for Q2\n",
      "Epoch: 18, Avg Val Loss: 0.6875, Avg Val F1 Score: 0.543\n",
      "Epoch: 19, Avg Val Loss: 0.6881, Avg Val F1 Score: 0.584\n",
      "✅ Best model saved for Q2\n",
      "Epoch: 20, Avg Val Loss: 0.6854, Avg Val F1 Score: 0.6432\n",
      "✅ Best model saved for Q2\n",
      "Epoch: 21, Avg Val Loss: 0.6801, Avg Val F1 Score: 0.6253\n",
      "Epoch: 22, Avg Val Loss: 0.6876, Avg Val F1 Score: 0.6062\n",
      "Epoch: 23, Avg Val Loss: 0.6868, Avg Val F1 Score: 0.6342\n",
      "Epoch: 24, Avg Val Loss: 0.6849, Avg Val F1 Score: 0.6253\n",
      "Epoch: 25, Avg Val Loss: 0.677, Avg Val F1 Score: 0.6342\n",
      "Epoch: 26, Avg Val Loss: 0.6681, Avg Val F1 Score: 0.6449\n",
      "✅ Best model saved for Q2\n",
      "Epoch: 27, Avg Val Loss: 0.6632, Avg Val F1 Score: 0.6665\n",
      "✅ Best model saved for Q2\n",
      "Epoch: 28, Avg Val Loss: 0.6657, Avg Val F1 Score: 0.6733\n",
      "✅ Best model saved for Q2\n",
      "Epoch: 29, Avg Val Loss: 0.6695, Avg Val F1 Score: 0.6467\n",
      "Epoch: 30, Avg Val Loss: 0.6709, Avg Val F1 Score: 0.6608\n",
      "Training complete.\n",
      "torch.Size([310, 14, 126]) torch.Size([310])\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.7029, Avg Val F1 Score: 0.3503\n",
      "✅ Best model saved for Q3\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.6986, Avg Val F1 Score: 0.4012\n",
      "✅ Best model saved for Q3\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.6935, Avg Val F1 Score: 0.4901\n",
      "✅ Best model saved for Q3\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.6879, Avg Val F1 Score: 0.4892\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.6821, Avg Val F1 Score: 0.5279\n",
      "✅ Best model saved for Q3\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.677, Avg Val F1 Score: 0.5315\n",
      "✅ Best model saved for Q3\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.6697, Avg Val F1 Score: 0.6445\n",
      "✅ Best model saved for Q3\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.6642, Avg Val F1 Score: 0.6093\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.6589, Avg Val F1 Score: 0.6093\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.6557, Avg Val F1 Score: 0.658\n",
      "✅ Best model saved for Q3\n",
      "Epoch: 11, Avg Val Loss: 0.6536, Avg Val F1 Score: 0.658\n",
      "Epoch: 12, Avg Val Loss: 0.6492, Avg Val F1 Score: 0.6707\n",
      "✅ Best model saved for Q3\n",
      "Epoch: 13, Avg Val Loss: 0.6461, Avg Val F1 Score: 0.6613\n",
      "Epoch: 14, Avg Val Loss: 0.6449, Avg Val F1 Score: 0.6372\n",
      "Epoch: 15, Avg Val Loss: 0.6404, Avg Val F1 Score: 0.5994\n",
      "Epoch: 16, Avg Val Loss: 0.641, Avg Val F1 Score: 0.6091\n",
      "Epoch: 17, Avg Val Loss: 0.6408, Avg Val F1 Score: 0.6361\n",
      "Epoch: 18, Avg Val Loss: 0.647, Avg Val F1 Score: 0.6091\n",
      "Epoch: 19, Avg Val Loss: 0.6512, Avg Val F1 Score: 0.5976\n",
      "Epoch: 20, Avg Val Loss: 0.6541, Avg Val F1 Score: 0.5882\n",
      "Epoch: 21, Avg Val Loss: 0.6519, Avg Val F1 Score: 0.6152\n",
      "Epoch: 22, Avg Val Loss: 0.6508, Avg Val F1 Score: 0.6152\n",
      "Epoch: 23, Avg Val Loss: 0.6567, Avg Val F1 Score: 0.6152\n",
      "Epoch: 24, Avg Val Loss: 0.6564, Avg Val F1 Score: 0.607\n",
      "Epoch: 25, Avg Val Loss: 0.6535, Avg Val F1 Score: 0.607\n",
      "Epoch: 26, Avg Val Loss: 0.6529, Avg Val F1 Score: 0.6264\n",
      "Epoch: 27, Avg Val Loss: 0.646, Avg Val F1 Score: 0.6264\n",
      "Early stopping triggered.\n",
      "torch.Size([310, 14, 126]) torch.Size([310])\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.6743, Avg Val F1 Score: 0.4603\n",
      "✅ Best model saved for S2\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.6721, Avg Val F1 Score: 0.4603\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.6697, Avg Val F1 Score: 0.4603\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.6661, Avg Val F1 Score: 0.4603\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.6627, Avg Val F1 Score: 0.4603\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.6593, Avg Val F1 Score: 0.4603\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.6557, Avg Val F1 Score: 0.4603\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.6529, Avg Val F1 Score: 0.4603\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.6501, Avg Val F1 Score: 0.4603\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.6486, Avg Val F1 Score: 0.4603\n",
      "Epoch: 11, Avg Val Loss: 0.6454, Avg Val F1 Score: 0.4603\n",
      "Epoch: 12, Avg Val Loss: 0.6422, Avg Val F1 Score: 0.4603\n",
      "Epoch: 13, Avg Val Loss: 0.6366, Avg Val F1 Score: 0.4603\n",
      "Epoch: 14, Avg Val Loss: 0.6296, Avg Val F1 Score: 0.4603\n",
      "Epoch: 15, Avg Val Loss: 0.6218, Avg Val F1 Score: 0.4603\n",
      "Epoch: 16, Avg Val Loss: 0.6161, Avg Val F1 Score: 0.53\n",
      "✅ Best model saved for S2\n",
      "Epoch: 17, Avg Val Loss: 0.6061, Avg Val F1 Score: 0.5841\n",
      "✅ Best model saved for S2\n",
      "Epoch: 18, Avg Val Loss: 0.5946, Avg Val F1 Score: 0.5841\n",
      "Epoch: 19, Avg Val Loss: 0.5829, Avg Val F1 Score: 0.5767\n",
      "Epoch: 20, Avg Val Loss: 0.5716, Avg Val F1 Score: 0.5993\n",
      "✅ Best model saved for S2\n",
      "Epoch: 21, Avg Val Loss: 0.5623, Avg Val F1 Score: 0.6038\n",
      "✅ Best model saved for S2\n",
      "Epoch: 22, Avg Val Loss: 0.5697, Avg Val F1 Score: 0.6038\n",
      "Epoch: 23, Avg Val Loss: 0.562, Avg Val F1 Score: 0.6536\n",
      "✅ Best model saved for S2\n",
      "Epoch: 24, Avg Val Loss: 0.5571, Avg Val F1 Score: 0.6536\n",
      "Epoch: 25, Avg Val Loss: 0.551, Avg Val F1 Score: 0.6654\n",
      "✅ Best model saved for S2\n",
      "Epoch: 26, Avg Val Loss: 0.5561, Avg Val F1 Score: 0.6654\n",
      "Epoch: 27, Avg Val Loss: 0.5516, Avg Val F1 Score: 0.6462\n",
      "Epoch: 28, Avg Val Loss: 0.5477, Avg Val F1 Score: 0.7016\n",
      "✅ Best model saved for S2\n",
      "Epoch: 29, Avg Val Loss: 0.5478, Avg Val F1 Score: 0.7129\n",
      "✅ Best model saved for S2\n",
      "Epoch: 30, Avg Val Loss: 0.559, Avg Val F1 Score: 0.6501\n",
      "Training complete.\n",
      "torch.Size([310, 14, 126]) torch.Size([310])\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 0.7285, Avg Val F1 Score: 0.2333\n",
      "✅ Best model saved for S3\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 0.7243, Avg Val F1 Score: 0.2333\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 0.7186, Avg Val F1 Score: 0.2333\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 0.7111, Avg Val F1 Score: 0.2333\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 0.7039, Avg Val F1 Score: 0.2333\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 0.6954, Avg Val F1 Score: 0.3007\n",
      "✅ Best model saved for S3\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 0.6865, Avg Val F1 Score: 0.4543\n",
      "✅ Best model saved for S3\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 0.6763, Avg Val F1 Score: 0.7217\n",
      "✅ Best model saved for S3\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 0.6626, Avg Val F1 Score: 0.6606\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 0.6489, Avg Val F1 Score: 0.5469\n",
      "Epoch: 11, Avg Val Loss: 0.64, Avg Val F1 Score: 0.5469\n",
      "Epoch: 12, Avg Val Loss: 0.6316, Avg Val F1 Score: 0.5091\n",
      "Epoch: 13, Avg Val Loss: 0.6238, Avg Val F1 Score: 0.5815\n",
      "Epoch: 14, Avg Val Loss: 0.6171, Avg Val F1 Score: 0.5815\n",
      "Epoch: 15, Avg Val Loss: 0.6084, Avg Val F1 Score: 0.5815\n",
      "Epoch: 16, Avg Val Loss: 0.5988, Avg Val F1 Score: 0.5815\n",
      "Epoch: 17, Avg Val Loss: 0.5912, Avg Val F1 Score: 0.6135\n",
      "Epoch: 18, Avg Val Loss: 0.5901, Avg Val F1 Score: 0.6688\n",
      "Epoch: 19, Avg Val Loss: 0.5952, Avg Val F1 Score: 0.6606\n",
      "Epoch: 20, Avg Val Loss: 0.5958, Avg Val F1 Score: 0.6035\n",
      "Epoch: 21, Avg Val Loss: 0.5945, Avg Val F1 Score: 0.6593\n",
      "Epoch: 22, Avg Val Loss: 0.5934, Avg Val F1 Score: 0.6593\n",
      "Epoch: 23, Avg Val Loss: 0.5969, Avg Val F1 Score: 0.6186\n",
      "Early stopping triggered.\n",
      "Epoch 1: Warm-up LR = 0.000010\n",
      "Epoch: 1, Avg Val Loss: 1.0921, Avg Val F1 Score: 0.3226\n",
      "✅ Best model saved for S1\n",
      "Epoch 2: Warm-up LR = 0.000020\n",
      "Epoch: 2, Avg Val Loss: 1.0897, Avg Val F1 Score: 0.3158\n",
      "Epoch 3: Warm-up LR = 0.000030\n",
      "Epoch: 3, Avg Val Loss: 1.0861, Avg Val F1 Score: 0.3289\n",
      "✅ Best model saved for S1\n",
      "Epoch 4: Warm-up LR = 0.000040\n",
      "Epoch: 4, Avg Val Loss: 1.0818, Avg Val F1 Score: 0.3509\n",
      "✅ Best model saved for S1\n",
      "Epoch 5: Warm-up LR = 0.000050\n",
      "Epoch: 5, Avg Val Loss: 1.0768, Avg Val F1 Score: 0.4416\n",
      "✅ Best model saved for S1\n",
      "Epoch 6: Warm-up LR = 0.000060\n",
      "Epoch: 6, Avg Val Loss: 1.0719, Avg Val F1 Score: 0.4252\n",
      "Epoch 7: Warm-up LR = 0.000070\n",
      "Epoch: 7, Avg Val Loss: 1.0667, Avg Val F1 Score: 0.3961\n",
      "Epoch 8: Warm-up LR = 0.000080\n",
      "Epoch: 8, Avg Val Loss: 1.0612, Avg Val F1 Score: 0.4417\n",
      "✅ Best model saved for S1\n",
      "Epoch 9: Warm-up LR = 0.000090\n",
      "Epoch: 9, Avg Val Loss: 1.0532, Avg Val F1 Score: 0.4694\n",
      "✅ Best model saved for S1\n",
      "Epoch 10: Warm-up LR = 0.000100\n",
      "Epoch: 10, Avg Val Loss: 1.0469, Avg Val F1 Score: 0.458\n",
      "Epoch: 11, Avg Val Loss: 1.0394, Avg Val F1 Score: 0.4755\n",
      "✅ Best model saved for S1\n",
      "Epoch: 12, Avg Val Loss: 1.0342, Avg Val F1 Score: 0.4325\n",
      "Epoch: 13, Avg Val Loss: 1.0276, Avg Val F1 Score: 0.4582\n",
      "Epoch: 14, Avg Val Loss: 1.0196, Avg Val F1 Score: 0.4582\n",
      "Epoch: 15, Avg Val Loss: 1.0133, Avg Val F1 Score: 0.4342\n",
      "Epoch: 16, Avg Val Loss: 1.0087, Avg Val F1 Score: 0.4342\n",
      "Epoch: 17, Avg Val Loss: 1.0049, Avg Val F1 Score: 0.4786\n",
      "✅ Best model saved for S1\n",
      "Epoch: 18, Avg Val Loss: 0.9981, Avg Val F1 Score: 0.5162\n",
      "✅ Best model saved for S1\n",
      "Epoch: 19, Avg Val Loss: 0.9904, Avg Val F1 Score: 0.5179\n",
      "✅ Best model saved for S1\n",
      "Epoch: 20, Avg Val Loss: 0.9845, Avg Val F1 Score: 0.5179\n",
      "Epoch: 21, Avg Val Loss: 0.979, Avg Val F1 Score: 0.4722\n",
      "Epoch: 22, Avg Val Loss: 0.9768, Avg Val F1 Score: 0.4957\n",
      "Epoch: 23, Avg Val Loss: 0.976, Avg Val F1 Score: 0.5257\n",
      "✅ Best model saved for S1\n",
      "Epoch: 24, Avg Val Loss: 0.9795, Avg Val F1 Score: 0.505\n",
      "Epoch: 25, Avg Val Loss: 0.9771, Avg Val F1 Score: 0.5067\n",
      "Epoch: 26, Avg Val Loss: 0.9767, Avg Val F1 Score: 0.517\n",
      "Epoch: 27, Avg Val Loss: 0.9694, Avg Val F1 Score: 0.5067\n",
      "Epoch: 28, Avg Val Loss: 0.9749, Avg Val F1 Score: 0.4977\n",
      "Epoch: 29, Avg Val Loss: 0.9786, Avg Val F1 Score: 0.4845\n",
      "Epoch: 30, Avg Val Loss: 0.9788, Avg Val F1 Score: 0.4688\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "from models.fftformer import *\n",
    "from train import *\n",
    "from prediction import *\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "binary_preds = {}\n",
    "binary_f1 = {}\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "hidden_dim = 128\n",
    "num_heads = 64\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "\n",
    "for col in targets_binary:\n",
    "    y_train = torch.tensor(np.array([[y[col]] for y in Y_seq]), dtype=torch.long).view(-1)\n",
    "    \n",
    "    print(X_train.shape, y_train.shape)\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    model_bin = FFTformerClassifier(input_dim=X_train.shape[-1], num_classes=2, hidden_dim=int(hidden_dim), num_heads=int(num_heads))\n",
    "    binary_f1[col] = train_model(model_bin, dataloader, nn.CrossEntropyLoss(), optim.Adam(model_bin.parameters(), lr=0.0001), col = col, epochs=30)\n",
    "    binary_preds[col] = predict(model_bin, torch.tensor(X_test, dtype=torch.float32), col)\n",
    "    f1_scores.append(binary_f1[col])\n",
    "\n",
    "y_multi_train = torch.tensor(np.array([[y[target_multiclass]] for y in Y_seq]), dtype=torch.long).view(-1)\n",
    "dataset_multi = TensorDataset(X_train, y_multi_train)\n",
    "dataloader_multi = DataLoader(dataset_multi, batch_size=32, shuffle=True)\n",
    "\n",
    "model_multi = FFTformerClassifier(input_dim=X_train.shape[-1], num_classes=3, hidden_dim=int(hidden_dim), num_heads=int(num_heads))\n",
    "multiclass_f1 = train_model(model_multi, dataloader_multi, nn.CrossEntropyLoss(), optim.Adam(model_multi.parameters(), lr=0.0001), col = 'S1', epochs=30)\n",
    "f1_scores.append(multiclass_f1)\n",
    "\n",
    "multiclass_pred = predict(model_multi, torch.tensor(X_test, dtype=torch.float32), 'S1')\n",
    "avg_f1 = sum(f1_scores) / len(f1_scores)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 제출 파일 생성 완료: submission_modify_model_hidden_32_head_32_num_layer_12_seq_14.csv\n"
     ]
    }
   ],
   "source": [
    "generate_submission(sample_submission, binary_preds, multiclass_pred, f'submission_modify_model_hidden_{hidden_dim}_head_{num_heads}_num_layer_12_seq_14.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
